{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/rhodes-byu/cs180-winter25/blob/main/notebooks/15-random-forests\n",
    ".ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Random Forests\n",
    "\n",
    "Random Forests are ensemble learning methods that build multiple decision trees and aggregate their predictions to improve accuracy and generalization. They are widely used in classification and regression tasks due to their robustness and ability to handle complex data structures.\n",
    "\n",
    "## Process of Random Forests\n",
    "\n",
    "### 1. Bootstrap Sampling\n",
    "Random Forests use **bootstrap aggregation (bagging)** to create diverse training datasets. Each tree in the forest is trained on a randomly sampled subset of the training data, drawn **with replacement**. This means some instances may appear multiple times in a subset, while others may be omitted.\n",
    "\n",
    "### 2. Random Feature Selection\n",
    "At each split in a tree, only a **random subset of features** is considered rather than all available features. This prevents dominance of strong predictors and encourages diversity among trees, reducing overfitting.\n",
    "\n",
    "### 3. Splitting Using Gini Impurity\n",
    "For classification problems, splits in the decision trees are typically determined by **Gini impurity**, which measures the probability of incorrect classification at a given node. The Gini impurity for a node is calculated as:\n",
    "\n",
    "$$\n",
    "G(I) = 1 - \\sum_{i=1}^{C} p_i^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ C $ is the number of classes,\n",
    "- $ p_i $ is the proportion of samples belonging to class $ i $ in the node.\n",
    "\n",
    "A lower Gini impurity value indicates a purer node, meaning the data points in that node belong mostly to a single class. The best split is chosen to minimize the weighted Gini impurity of the resulting child nodes.\n",
    "\n",
    "### 4. Other Hyperparameters\n",
    "Several additional arguments can be tuned to control the behavior of the forest:\n",
    "   - `n_estimators`: Number of trees in the forest.\n",
    "   - `max_features`: Number of randomly selected features at each split.\n",
    "   - `min_samples_split`: Minimum number of samples required to split an internal node.\n",
    "   - `min_samples_leaf`: Minimum number of samples required to be in a leaf node.\n",
    "   - `max_depth`: Limits tree growth to prevent overfitting.\n",
    "\n",
    "### 5. Generally Grown to Maximal Depth\n",
    "In many implementations, trees are **fully grown** until all leaves are pure (i.e., contain only a single class) or reach a minimum leaf size. Unlike pruning methods used in individual decision trees, Random Forests rely on ensemble averaging to reduce overfitting rather than restricting tree depth.\n",
    "\n",
    "### 6. Low Bias, Low Variance\n",
    "Random Forests strike a balance between bias and variance:\n",
    "   - **Low bias**: Trees have high capacity to learn complex patterns.\n",
    "   - **Low variance**: Averaging multiple decorrelated trees reduces overfitting and improves generalization.\n",
    "\n",
    "### 7. Minimizing Correlation Between Trees\n",
    "To maximize the benefits of ensemble learning, Random Forests aim to **minimize correlation** among trees. This is achieved through:\n",
    "   - **Bootstrap sampling**, which ensures different training data for each tree.\n",
    "   - **Random feature selection**, which forces trees to explore different parts of the feature space.\n",
    "\n",
    "By reducing tree correlation, the variance of the overall model is reduced, making it more robust to noise and improving predictive performance.\n",
    "\n",
    "### 8. Out-of-Bag (OOB) Points for Model Validation\n",
    "Since each tree in a Random Forest is trained on a **bootstrap sample**, about **37% of the training data** is **not used** to train a given tree. These **out-of-bag (OOB) samples** serve as a built-in validation set.\n",
    "\n",
    "- The OOB samples are used to evaluate model performance **without requiring a separate validation set**.\n",
    "- Each data point is predicted using only the trees **where it was not included in training**.\n",
    "- The OOB error, computed as the average error across these predictions, provides an **unbiased estimate of test error**.\n",
    "\n",
    "This method makes Random Forests highly efficient, as they internally estimate generalization performance without requiring cross-validation.\n",
    "\n",
    "### 9. Aggregating Predictions\n",
    "Random Forests aggregate the predictions from all individual trees to make a final prediction:\n",
    "\n",
    "- **Classification**: Each tree casts a vote for a class label, and the final prediction is determined by the **majority vote** (i.e., the class that receives the most votes) :contentReference[oaicite:0]{index=0}.\n",
    "- **Regression**: Each tree provides a numerical prediction, and the final prediction is obtained by computing the **average** of all tree predictions :contentReference[oaicite:1]{index=1}.\n",
    "\n",
    "This aggregation process enhances the model's robustness and accuracy by combining the strengths of multiple trees.\n",
    "\n",
    "## Benefits of Random Forests\n",
    "\n",
    "Random Forests offer several advantages that make them a popular choice for many machine learning tasks:\n",
    "\n",
    "- **Easy to use and provide a good baseline**: They work well with minimal tuning and can serve as a strong initial model.\n",
    "- **Scale invariant**: Unlike some models, Random Forests do not require feature scaling (e.g., standardization or normalization).\n",
    "- **Used for both classification and regression**: They are flexible and applicable to a wide range of supervised learning tasks.\n",
    "- **Feature importance measurement**: They provide estimates of feature importance, which can help with feature selection and model interpretability.\n",
    "- **Handle mixed variable types**: They work with both categorical and numerical features without requiring extensive preprocessing.\n",
    "- **Strong out-of-the-box performance**: Even with default hyperparameters, Random Forests often yield competitive results.\n",
    "- **Model non-linear relationships**: They can capture complex patterns and interactions between features without requiring explicit feature engineering.\n",
    "\n",
    "## Additional Uses of Random Forests\n",
    "\n",
    "Beyond traditional classification and regression, Random Forests have several additional applications:\n",
    "\n",
    "### 1. Generation of Random Forest Proximities\n",
    "Random Forests can compute **proximities** between data points by measuring how often two instances end up in the same leaf node across all trees in the forest. These proximity measures have various applications:\n",
    "\n",
    "   - **Cluster analysis**: By analyzing proximities, one can detect natural groupings in data without predefined clusters.\n",
    "   - **Anomaly detection**: Outliers often have low proximity to other data points, making it possible to flag unusual observations.\n",
    "   - **Data visualization (e.g., multidimensional scaling)**: Proximities can be used to project high-dimensional data into a lower-dimensional space for visualization.\n",
    "   - **Similarity-based recommendation systems**: Items or users with high proximity can be grouped to provide recommendations.\n",
    "\n",
    "### 2. Missing Data Imputation\n",
    "Since Random Forests can estimate proximity between samples, they can be used to **impute missing values** by averaging values from the most similar instances. This method often performs well, especially when missing data is **not missing completely at random (MCAR)**.\n",
    "\n",
    "### 3. Outlier Detection\n",
    "Random Forests can be used to **detect outliers** by analyzing proximity distributions. Data points that are consistently dissimilar to all others (i.e., have low average proximity to other points) can be flagged as potential anomalies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.base import clone\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tic-tac-toe dataset\n",
    "[Data Link Here](https://archive.ics.uci.edu/dataset/101/tic+tac+toe+endgame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_tac_toe = fetch_openml(name='tic-tac-toe', version=1, parser = 'auto')\n",
    "X = tic_tac_toe.data\n",
    "y = tic_tac_toe.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer-encoded values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inteter-valued encoding\n",
    "X_enc = X.apply(lambda x: pd.Categorical(x).codes if x.dtype == 'category' else x)\n",
    "X_enc_train, X_enc_test, y_train, y_test = train_test_split(X_enc, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_enc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the Random Forest on the Encoded data\n",
    "Check fit using the OOB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_enc = RandomForestClassifier(n_estimators = 100, random_state = 42, oob_score = True) # oob_score to estimate the generalization error\n",
    "rf_enc.fit(X_enc_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the underscrore_; this is the internally stored OOB accuracy\n",
    "rf_enc.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_enc = rf_enc.predict(X_enc_test)\n",
    "print(accuracy_score(y_test, y_pred_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "The mean decrease in Gini impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_enc.feature_importances_\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = np.sort(importances)[::-1], y = X_enc.columns[np.argsort(importances)][::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoded Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hot = pd.get_dummies(X)\n",
    "print(X_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hot_train, X_hot_test, y_train, y_test = train_test_split(X_hot, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 100, random_state = 42, oob_score = True)\n",
    "rf.fit(X_hot_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add blurb about OOB Score\n",
    "rf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_hot = rf.predict(X_hot_test)\n",
    "print(accuracy_score(y_test, y_pred_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add blurb about feature importance\n",
    "\n",
    "rf.feature_importances_\n",
    "X_hot.columns[np.argsort(rf.feature_importances_)[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = np.sort(rf.feature_importances_)[::-1], y = X_hot.columns[np.argsort(rf.feature_importances_)][::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of n_estimators (Number of Trees) on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [1, 5, 10, 50, 100, 500, 1000]:\n",
    "    start_time = time.time()\n",
    "    rf = RandomForestClassifier(n_estimators=n, random_state=42, oob_score=True)\n",
    "    rf.fit(X_hot_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"n = {n}: OOB Score = {rf.oob_score_}, Time to fit = {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misclassified Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_indices = y_test[y_pred_hot != y_test].index\n",
    "misclassified_points = X_hot_test.loc[misclassified_indices]\n",
    "misclassified_points[misclassified_points == 1].dropna(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.loc[misclassified_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximity-based Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating random forest proximities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/jakerhodes/RF-GAP-Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rfgap import RFGAP\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfgap_ob = RFGAP(oob_score = True, random_state = 42, non_zero_diagonal = True, force_symmetric = True)\n",
    "rfgap_ob.fit(X_hot_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proximities = rfgap_ob.get_proximities().toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne = tsne(n_components = 2, dissimilarity = 'precomputed', random_state = 42)\n",
    "tsne = TSNE(n_components = 2, metric = 'precomputed', init = 'random', random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tsne.fit_transform(1 - proximities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(x = embedding[:, 0], y = embedding[:, 1], color = y_train, title = 't-SNE of Tic-Tac-Toe Data',\n",
    "                 height = 400, width = 600,\n",
    "                 hover_data = {'X_hot': X_hot_train.index})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hot_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn to explore!\n",
    "\n",
    "Play around with the scatterplot, color by different variables and see if you can find any patterns relating to the labels.  \n",
    "Do any of the patterns coincide with the variable importances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Code Here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Decision Boundaries on T-SNE Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tsne = RandomForestClassifier(n_estimators = 100, random_state = 42, oob_score = True, class_weight = 'balanced')\n",
    "rf_tsne.fit(embedding, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tsne.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "X_vis = embedding\n",
    "\n",
    "rf_model_vis = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42,\n",
    "                                      class_weight = 'balanced', n_jobs = -2)\n",
    "rf_model_vis.fit(X_vis, y_train)\n",
    "\n",
    "x_min, x_max = X_vis[:, 0].min() - 1, X_vis[:, 0].max() + 1\n",
    "y_min, y_max = X_vis[:, 1].min() - 1, X_vis[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "Z = rf_model_vis.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = pd.Categorical(Z).codes\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap=ListedColormap([\"#FFAAAA\", \"#AAAAFF\"]))\n",
    "plt.scatter(X_vis[:, 0], X_vis[:, 1], c = pd.Categorical(y_train).codes, edgecolor = \"k\", cmap = ListedColormap([\"#FF0000\", \"#0000FF\"]))\n",
    "plt.title(\"Decision Boundaries of Random Forest\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
