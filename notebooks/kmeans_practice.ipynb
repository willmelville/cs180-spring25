{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFjCdWoBW2s4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the iris dataset\n",
        "\n",
        "data = load_iris(as_frame = True)\n",
        "df = data['data']\n",
        "target_names = data['target_names']\n",
        "df['species'] = [target_names[i] for i in data['target'].values]\n",
        "df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']"
      ],
      "metadata": {
        "id": "7tgNTYbWXDDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normally it's important to standardize your variables before kmeans, but since the petal and sepal lengths and widths are all measured in centimeters, we should be able to get away with not doing it this time.\n",
        "\n",
        "First up, let's plot all the variables we are going to use to cluster on colored by species. This will show us the clusters we hope to find with kmeans."
      ],
      "metadata": {
        "id": "MQO3HAt8XdpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df, hue = 'species')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W9hD-2JDXMDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the elbow method or silhouttes scores, we could estimate the optimal number of K's for our clusters, but if we use some feel we can go with K = 3 since there are three different species, so we'll start with K = 3."
      ],
      "metadata": {
        "id": "5bieY0rXYOOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize the kmeans model\n",
        "km = KMeans(n_clusters = 3)\n",
        "\n",
        "#fit the model to the data. We want to cluster on the petal lengths and widths and the sepal lengths and widths\n",
        "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values\n",
        "km.fit(X)\n",
        "\n",
        "#now define the cluster labels for each point in the data\n",
        "df['cluster_label'] = km.predict(X)\n",
        "\n",
        "#This step is optional, but the cluster labels are integers, so if we did the seaborn pairplot with hue=cluster_label,\n",
        "#it would assume a continuous hue. IF we want to replicate the results from our earlier plot when we colored by species, we need a discrete hue.\n",
        "#We can trick seaborn into thinking the integers are discrete by changing their data types to strings.\n",
        "df['cluster_label'] = df.cluster_label.astype(str)\n",
        "\n",
        "#now we can see how good our KMeans clustering did by plotting the same pairplot we did before but using our label instead of the species\n",
        "sns.pairplot(df, hue = 'cluster_label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "82EZOtyGXVp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The colors may not exactly match up because it is random which cluster gets defined first, second, and third, but the plots in the previous cell should show similar groupings as the earlier plots when we colored the points by species. KMeans was able to group the different iris species together!"
      ],
      "metadata": {
        "id": "vErXSYU1Zpe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's practice using the elbow method and silhoutte score to choose an optimal K. Obviously K = 3 is a great choice for this problem, so let's see if the elbow method and the silhoutte scores also find that K = 3 is a good choice."
      ],
      "metadata": {
        "id": "0nswckX5Z5mG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#elbow method first\n",
        "\n",
        "#initialize an empty list to store the within cluster sum of squares for various values of k\n",
        "wcss = []\n",
        "\n",
        "#loop through the values of k that you want to test. Here I choose to test K = 2 to 7\n",
        "for k in range(2, 8):\n",
        "\n",
        "  #define and fit a KMeans clusterer with k centroids\n",
        "  km = KMeans(n_clusters = k, random_state = 42)\n",
        "  km.fit(X)\n",
        "\n",
        "  #the within cluster sum of squares is an attribute in sklearns KMeans object class. The attribute is called inertia_\n",
        "  wcss.append(km.inertia_)\n",
        "\n",
        "#now we have calculated the different wcss values for the different values of K, so we just need to plot them and find the elbow\n",
        "plt.plot([k for k in range(2, 8)], wcss, 'b-x')\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('Sum of Squared Distances')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nDVJ3ycNZgME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks to me like K = 3 or 4 would be a good choice according to the elbow test."
      ],
      "metadata": {
        "id": "wTqoheiRa7Xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#next up, the silhouette score\n",
        "\n",
        "#initialize an empty list to store the scores\n",
        "sil_scores = []\n",
        "\n",
        "#loop through the values of K\n",
        "for k in range(2,8):\n",
        "\n",
        "  #fit a kmeans clusterer\n",
        "  km = KMeans(n_clusters = k, random_state = 42)\n",
        "  km.fit(X)\n",
        "\n",
        "  #predict the cluster labels\n",
        "  labels = km.predict(X)\n",
        "\n",
        "  #calculate silhouette score and append to the list\n",
        "  sil_scores.append(silhouette_score(X, labels))\n",
        "\n",
        "#plot the K values and the silhouette score. The optimal K is the one with max silhoutte score\n",
        "plt.plot([k for k in range(2,8)], sil_scores, 'b-x')\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eNUGwiIca5Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The silhoutte score in this case suggests that we only want two clusters, which is why sometimes it's important to have some data science feel. If we know there are three species of irises in the dataset, we should probably go with three clusters despite what the silhoutte score says."
      ],
      "metadata": {
        "id": "MGpSKJj8b7B9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9vuz_KSZbpV_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}