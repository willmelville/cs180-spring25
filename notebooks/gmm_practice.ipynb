{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ngz6AA5Ez38b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the iris dataset\n",
        "\n",
        "data = load_iris(as_frame = True)\n",
        "df = data['data']\n",
        "target_names = data['target_names']\n",
        "df['species'] = [target_names[i] for i in data['target'].values]\n",
        "df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']"
      ],
      "metadata": {
        "id": "-PHu_qh90H6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like in the KMeans practice notebook, we are not going to standardize the variables because we know they are all measured on the same scale, but also just like with KMeans it is usually a good idea to standardize your variables before fitting a guassian mixture model.\n",
        "\n",
        "Let's again start by plotting the variables that we are clustering on and coloring the points by species. Those are the clusters we will hope to learn with our Gaussian mixture model."
      ],
      "metadata": {
        "id": "tXff0Mfv0K6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df, hue = 'species')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FEZprCdZ0KlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will fit a guassian mixture model with 3 components (one for each species). Then we will get the cluster labels from our gmm, and recreate the plot from above but this time color the points by our cluster labels. Hopefully we find similar clusters as we get when we color by species."
      ],
      "metadata": {
        "id": "YFZYE4WD0m_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize the gaussian mixture model\n",
        "gmm = GaussianMixture(n_components = 3)\n",
        "\n",
        "#fit the model to the data\n",
        "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values\n",
        "gmm.fit(X)\n",
        "\n",
        "#define the cluster labels for each point. Note, we could also run predict_proba to get the probability of a point being in each cluster instead of the cluster label. We can't do that with KMeans\n",
        "df['cluster_label'] = gmm.predict(X)\n",
        "\n",
        "#This step is optional, but the cluster labels are integers, so if we did the seaborn pairplot with hue=cluster_label,\n",
        "#it would assume a continuous hue. IF we want to replicate the results from our earlier plot when we colored by species, we need a discrete hue.\n",
        "#We can trick seaborn into thinking the integers are discrete by changing their data types to strings.\n",
        "df['cluster_label'] = df.cluster_label.astype(str)\n",
        "\n",
        "#now we can see how good our KMeans clustering did by plotting the same pairplot we did before but using our label instead of the species\n",
        "sns.pairplot(df, hue = 'cluster_label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5qA0PIri0kKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we hoped, our GMM learned similar clusters as the ones we see when we color the points by species.\n",
        "\n",
        "Since a GMM is a probability distribution, you can also simulate/generate new points, which we illustrate in the following cell."
      ],
      "metadata": {
        "id": "wfENKEds1Tzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#simulate 1000 new points\n",
        "sim_irises, labels = gmm.sample(1000)\n",
        "sim_df = pd.DataFrame(data = sim_irises, columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
        "sim_df['label'] = labels\n",
        "sim_df.label = sim_df.label.astype(str)\n",
        "\n",
        "sns.pairplot(sim_df, hue = 'label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vWnO5LMZ1E2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A8S-o8PS1zX7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}