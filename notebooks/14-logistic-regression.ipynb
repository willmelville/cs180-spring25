{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhodes-byu/cs180-winter25/blob/main/notebooks/14-logistic-regression\n",
        ".ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic Regression\n",
        "\n",
        "Logistic regression is a fundamental statistical method used for binary classification tasks, where the goal is to predict one of two possible outcomes based on input variables. It is widely utilized in fields like machine learning, medical research, and social sciences due to its simplicity and interpretability.\n",
        "\n",
        "## Assumptions and Limitations\n",
        "\n",
        "Logistic regression relies on several assumptions:\n",
        "\n",
        "- **Independence of observations:** Each sample should be independent.\n",
        "- **Linear relationship with log-odds:** Features should have a linear relationship with the log-odds of the outcome.\n",
        "- **No multicollinearity:** Strong correlations between predictors can distort coefficient estimates.\n",
        "\n",
        "However, logistic regression has limitations. It struggles with highly non-linear decision boundaries and may not perform well when dealing with large numbers of irrelevant features. In such cases, feature engineering or more complex models may be needed.\n",
        "\n",
        "## Mathematical Background\n",
        "\n",
        "At its core, logistic regression models the probability that a given input belongs to a particular class. Unlike linear regression, which predicts continuous outcomes, logistic regression constrains predictions to a probability range between 0 and 1. A linear model, if applied directly to classification, could produce outputs outside this range, making it unsuitable for probability estimation.\n",
        "\n",
        "### The Logistic Function\n",
        "\n",
        "The logistic function, also known as the sigmoid function, is defined as:\n",
        "\n",
        "$$\n",
        "\\sigma(t) = \\frac{1}{1 + e^{-t}}\n",
        "$$\n",
        "\n",
        "This function maps any real-valued number into the (0, 1) interval, making it suitable for probability estimation. It is a smooth, S-shaped function that asymptotically approaches 0 and 1, ensuring that probability values are well-defined.\n",
        "\n",
        "### Model Representation\n",
        "\n",
        "In logistic regression, the linear combination of input features $X$ and their corresponding coefficients $\\boldsymbol{\\beta}$ is passed through the logistic function to predict the probability $p$ of the positive class:\n",
        "\n",
        "$$\n",
        "p = \\sigma(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_n X_n)\n",
        "$$\n",
        "\n",
        "or equivalently using summation notation:\n",
        "\n",
        "$$\n",
        "p = \\sigma \\left( \\sum_{j=0}^{n} \\beta_j X_j \\right)\n",
        "$$\n",
        "\n",
        "Alternatively, using matrix notation:\n",
        "\n",
        "$$\n",
        "p = \\sigma(\\mathbf{X} \\boldsymbol{\\beta})\n",
        "$$\n",
        "\n",
        "\n",
        "where $\\mathbf{X}$ is the feature matrix and $\\boldsymbol{\\beta}$ is the coefficient vector. Here, $\\beta_0$ is the intercept term, and $\\beta_1, \\beta_2, \\ldots, \\beta_n$ are the coefficients for each feature.\n",
        "\n",
        "### Connection to the Binomial Distribution\n",
        "\n",
        "Logistic regression is based on the assumption that the dependent variable follows a **binomial distribution**. Given a binary outcome $y_i$ that takes values 0 or 1, the probability of success ($y_i = 1$) is modeled as:\n",
        "\n",
        "$$\n",
        "P(y_i = 1 | X_i) = p_i = \\sigma(\\mathbf{X}_i \\boldsymbol{\\beta})\n",
        "$$\n",
        "\n",
        "Similarly, the probability of failure ($y_i = 0$) is:\n",
        "\n",
        "$$\n",
        "P(y_i = 0 | X_i) = 1 - p_i\n",
        "$$\n",
        "\n",
        "or\n",
        "\n",
        "$$\n",
        "P(y_i = 0 | X_i) = 1 - \\sigma(\\mathbf{X}_i \\boldsymbol{\\beta})\n",
        "$$\n",
        "\n",
        "The binomial probability mass function (PMF) is given by:\n",
        "\n",
        "$$\n",
        "P(y_i | X_i) = p_i^{y_i} (1 - p_i)^{(1 - y_i)}\n",
        "$$\n",
        "\n",
        "Since each observation follows a Bernoulli distribution (a special case of the binomial distribution with a single trial), the likelihood function is derived from the product of Bernoulli probabilities across all observations. This leads to the formulation of **maximum likelihood estimation (MLE)** for parameter optimization.\n",
        "\n",
        "\n",
        "### **Maximum Likelihood Estimation (MLE)**\n",
        "\n",
        "To estimate the parameters $ \\boldsymbol{\\beta} $ in logistic regression, we use **maximum likelihood estimation (MLE)**. The goal of MLE is to find the values of $ \\boldsymbol{\\beta} $ that maximize the probability of observing the given dataset.\n",
        "\n",
        "Since logistic regression assumes that each observation follows a **Bernoulli distribution**, the likelihood function for a dataset of $ n $ independent observations is given by:\n",
        "\n",
        "$$\n",
        "L(\\boldsymbol{\\beta}) = \\prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{(1 - y_i)}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ p_i = \\sigma(\\mathbf{X}_i \\boldsymbol{\\beta}) $ is the predicted probability of the positive class for observation $ i $,\n",
        "- $ y_i $ is the actual class label (0 or 1).\n",
        "\n",
        "Taking the natural logarithm of the likelihood function (for easier optimization), we obtain the **log-likelihood function**:\n",
        "\n",
        "$$\n",
        "\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[ y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right]\n",
        "$$\n",
        "\n",
        "Maximizing this function with respect to $ \\boldsymbol{\\beta} $ provides the best-fitting parameters for the logistic regression model.\n",
        "\n",
        "Unlike linear regression, where we obtain a closed-form solution for $ \\boldsymbol{\\beta} $ using the normal equations, logistic regression requires **numerical optimization techniques** (such as gradient descent) to optimize the log-likelihood function.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Odds and Log-Odds\n",
        "\n",
        "The odds represent the ratio of the probability of the event occurring to it not occurring:\n",
        "\n",
        "$$\n",
        "\\text{Odds} = \\frac{p}{1 - p}\n",
        "$$\n",
        "\n",
        "Taking the natural logarithm of the odds yields the log-odds (or logit):\n",
        "\n",
        "$$\n",
        "\\text{Logit}(p) = \\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_n X_n\n",
        "$$\n",
        "\n",
        "### Interpretation of Coefficients\n",
        "\n",
        "This linear relationship between the log-odds and the input features forms the basis of logistic regression. The coefficients $\\beta_j$ in logistic regression can be interpreted as follows:\n",
        "\n",
        "- A **positive** coefficient $\\beta_j$ means that an increase in $X_j$ increases the log-odds of the event occurring, meaning that the probability of the event occurring increases.\n",
        "- A **negative** coefficient $\\beta_j$ means that an increase in $X_j$ decreases the log-odds of the event occurring, meaning that the probability of the event occurring decreases.\n",
        "- The **magnitude** of $\\beta_j$ determines the strength of the effect that $X_j$ has on the log-odds.\n",
        "- The exponentiated coefficient, $e^{\\beta_j}$, represents the **odds ratio**, which tells us how the odds of the event change for a one-unit increase in $X_j$. Specifically:\n",
        "  - If $e^{\\beta_j} > 1$, the odds of the event increase.\n",
        "  - If $e^{\\beta_j} < 1$, the odds of the event decrease.\n",
        "  - If $e^{\\beta_j} = 1$, $X_j$ has no effect on the odds.\n",
        "\n",
        "For example, if $\\beta_j = 0.7$, then $e^{0.7} \\approx 2.01$, meaning that a one-unit increase in $X_j$ doubles the odds of the event occurring.\n",
        "\n",
        "These interpretations make logistic regression a useful tool in various domains. In medical research, for instance, $\\beta_j$ values can help assess the impact of a risk factor (e.g., smoking) on the likelihood of disease occurrence. In marketing, they can reveal how different factors (e.g., ad spend) influence the probability of a customer making a purchase. The ability to directly interpret coefficients in terms of odds ratios makes logistic regression a highly valuable and intuitive statistical modeling approach.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-34pKjTf00v"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ektv5SG_OT3G"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "X = iris.data\n",
        "y = (iris.target == 2).astype(int)  # Binary target: 1 if 'Iris-Virginica', else 0\n",
        "\n",
        "features = iris.feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHRJzIEMHV5K"
      },
      "source": [
        "Create a new variable called y. For each flower in the dataset, if it's an 'Iris-Virginica' (represented by the number 2), assign a value of 1 to y. Otherwise, assign a value of 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y, random_state=1990)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "sns.countplot(x = y_train, ax = ax[0])\n",
        "sns.countplot(x = y_test, ax = ax[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predicted probabilities\n",
        "y_pred_prob = model.predict_proba(X_test)\n",
        "np.round(y_pred_prob[:10], 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJKme_3oijEq"
      },
      "source": [
        "The coefficients $( \\beta )$ in logistic regression indicate the direction and magnitude of the relationship between each feature and the log-odds of the outcome. A positive coefficient suggests that as the feature value increases, the log-odds of the outcome occurring increase, and vice versa for a negative coefficient.\n",
        "\n",
        "To access the coefficients in the trained model:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX2_UPTjixyF",
        "outputId": "86d43538-d1c9-46da-ba85-15956230a8a6"
      },
      "outputs": [],
      "source": [
        "# Intercept\n",
        "intercept = model.intercept_\n",
        "print(f'Intercept: {intercept}')\n",
        "\n",
        "# Coefficients\n",
        "coefficients = model.coef_\n",
        "print(f'Coefficients: {coefficients}')\n",
        "\n",
        "# Feature names\n",
        "print(f'Features: {features}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Model Coefficients\n",
        "\n",
        "Understanding these coefficients can provide valuable insights into the influence of each feature on the predicted outcome.\n",
        "\n",
        "### Intercept (`model.intercept_`)\n",
        "\n",
        "#### Meaning  \n",
        "The intercept represents the **log-odds** of the target variable (*Iris-Virginica* in this case) being 1 when all predictor variables (*sepal length, sepal width, petal length, petal width*) are 0.\n",
        "\n",
        "#### In the Code  \n",
        "```python\n",
        "intercept = model.intercept_\n",
        "```\n",
        "This retrieves the intercept value.\n",
        "\n",
        "#### Interpretation  \n",
        "- The intercept is often less practically meaningful in real-world scenarios, especially when predictor values of 0 are unrealistic (such as in flower measurements).  \n",
        "- It serves as the **baseline log-odds** before considering the impact of the predictor variables.\n",
        "\n",
        "### Coefficients (`model.coef_`)\n",
        "\n",
        "#### Meaning  \n",
        "Each coefficient corresponds to a specific predictor variable. They represent the **change in log-odds** of the target variable for a **one-unit change** in the corresponding predictor, holding other predictors constant.\n",
        "\n",
        "$$\n",
        "\\text{Logit}(p) = \\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_n X_n\n",
        "$$\n",
        "\n",
        "#### In the Code  \n",
        "```python\n",
        "coefficients = model.coef_\n",
        "```\n",
        "This retrieves the coefficients as an array. Since there are 4 features, you'll get 4 coefficients.\n",
        "\n",
        "#### Interpretation  \n",
        "- **Positive Coefficient**: A positive coefficient means that an increase in the predictor variable **increases** the log-odds of the target being 1 (*Iris-Virginica*).  \n",
        "- **Negative Coefficient**: A negative coefficient means that an increase in the predictor variable **decreases** the log-odds of the target being 1.  \n",
        "- **Magnitude**: The larger the absolute value of a coefficient, the stronger its influence on the outcome.\n",
        "\n",
        "#### Example  \n",
        "If the coefficient for **petal length** is `2.5`, this means:  \n",
        "> For every **one-unit increase** in petal length, the **log-odds** of the flower being *Virginica* increase by `2.5`, assuming other features remain constant. That is, the odds of *Virginica* prediction increases by $e^{2.5} \\approx 12.18$.\n",
        "\n",
        "### Important Considerations  \n",
        "\n",
        "#### Log-Odds vs. Probabilities  \n",
        "- These coefficients are in terms of **log-odds**, **not probabilities**.  \n",
        "- To obtain probabilities, apply the **sigmoid function**\n",
        "  The model performs this internally for predictions.\n",
        "\n",
        "#### Feature Scaling  \n",
        "- If your features have **vastly different scales**, consider **scaling them** before training the model.  \n",
        "- This helps with coefficient interpretation and ensures a more stable model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iris.feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "sns.scatterplot(x = X[:, 2], y = X[:, 3], hue = y, palette = 'viridis', ax = ax[0])\n",
        "ax[0].set_xlabel('Petal Length (cm)')\n",
        "ax[0].set_ylabel('Petal Width (cm)')\n",
        "ax[0].set_title('Scatter Plot of Petal Length vs Petal Width')\n",
        "\n",
        "sns.scatterplot(x = X[:, 0], y = X[:, 1], hue = y, palette = 'viridis', ax = ax[1])\n",
        "ax[1].set_xlabel('Sepal Length (cm)')\n",
        "ax[1].set_ylabel('Sepal Width (cm)')\n",
        "ax[1].set_title('Scatter Plot of Sepal Length vs Sepal Width')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example Boundary Point\n",
        "example_point = np.array([[6.0, 3.5, 5.0, 1.7]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weighted_sum = np.dot(example_point, coefficients.T) + intercept\n",
        "probability = 1 / (1 + np.exp(-weighted_sum))\n",
        "\n",
        "print('Weighted Sum: ', weighted_sum)\n",
        "print('Probability of Virginica: ', probability)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Point to Predict\n",
        "example_point = [7.0, 3.0, 6.5, 2.3]\n",
        "\n",
        "weighted_sum = np.dot(example_point, coefficients.T) + intercept\n",
        "probability = 1 / (1 + np.exp(-weighted_sum))\n",
        "\n",
        "print('Weighted Sum: ', weighted_sum)\n",
        "print('Probability of Virginica: ', probability)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the mesh grid for Features 2 and 3 (since we can only plot 2D)\n",
        "x_min, x_max = X[:, 2].min() - 1, X[:, 2].max() + 1  # Feature 2\n",
        "y_min, y_max = X[:, 3].min() - 1, X[:, 3].max() + 1  # Feature 3\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
        "                     np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "# Repeat entire dataset for prediction, replacing Features 2 and 3 with mesh values\n",
        "X_grid = np.tile(X, (xx.ravel().shape[0] // X.shape[0] + 1, 1))[:xx.ravel().shape[0], :]\n",
        "X_grid[:, 2] = xx.ravel()  # Feature 2 varies\n",
        "X_grid[:, 3] = yy.ravel()  # Feature 3 varies\n",
        "\n",
        "# Predict the class for each point in the mesh grid\n",
        "Z = model.predict(X_grid)\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.coolwarm)\n",
        "\n",
        "# Scatter plot for the original data points (only Features 2 and 3)\n",
        "plt.scatter(X_test[:, 2], X_test[:, 3], c = y_test, edgecolors='k', marker='o', s=20, cmap=plt.cm.coolwarm)\n",
        "\n",
        "plt.xlabel('Feature 2')\n",
        "plt.ylabel('Feature 3')\n",
        "plt.title('Decision Boundary of Logistic Regression Model')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jDMAyJtme8B"
      },
      "outputs": [],
      "source": [
        "'''Steps To Calculate Probability of a Class given Logistic Model:\n",
        "\n",
        "### Gather the Intercept and Coefficients: You already have these from your trained model:\n",
        "\n",
        "intercept = model.intercept_    # -12.83 in this case\n",
        "   coefficients = model.coef_   # An array of coefficients\n",
        "\n",
        "### Input Feature Values: Let's assume you have a new set of feature values for an Iris flower:\n",
        "\n",
        "new_features = [sepal_length, sepal_width, petal_length, petal_width]\n",
        "\n",
        "### Replace these placeholders with the actual values you want to predict for.\n",
        "\n",
        "### Calculate the Linear Combination: This involves multiplying each feature value by its corresponding coefficient and adding them together along with the intercept:\n",
        "\n",
        "linear_combination = intercept + coefficients[0][0] * new_features[0] + \\\n",
        "                          coefficients[0][1] * new_features[1] + \\\n",
        "                          coefficients[0][2] * new_features[2] + \\\n",
        "                          coefficients[0][3] * new_features[3]\n",
        "\n",
        "### Make sure to adjust the indices of coefficients if you have more or fewer features.\n",
        "\n",
        "### Apply the Logistic Function: This converts the linear combination (log-odds) into a probability between 0 and 1:\n",
        "\n",
        "probability = 1 / (1 + np.exp(-linear_combination))\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAUIzKj9mXVO"
      },
      "source": [
        "**Interpretation**:\n",
        "\n",
        "The resulting probability value represents the model's predicted probability of the Iris flower with the given features belonging to the 'Iris-Virginica' class.\n",
        "If probability is greater than 0.5, the model predicts the flower as 'Iris-Virginica'.\n",
        "If probability is less than 0.5, the model predicts the flower as not 'Iris-Virginica'.\n",
        "\n",
        "**Example**:\n",
        "\n",
        "Let's say your new_features are [6.0, 3.0, 5.0, 2.0]. The calculation would look like this:\n",
        "\n",
        "\n",
        "linear_combination = -12.83 + coefficients[0][0] * 6.0 + coefficients[0][1] * 3.0 + \\\n",
        "                      coefficients[0][2] * 5.0 + coefficients[0][3] * 2.0\n",
        "probability = 1 / (1 + np.exp(-linear_combination))\n",
        "\n",
        "Replace coefficients[0][0], coefficients[0][1], etc., with the actual coefficient values from your model. The resulting probability would be the model's prediction for this specific set of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxkgycdWmr6s"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
