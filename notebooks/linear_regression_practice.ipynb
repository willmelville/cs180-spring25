{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYPx1qMZursn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin our practice by fitting a linear regression model using the linear algebra formula we derived in class: $\\beta^* = (X^TX)^{-1}X^Ty$. We'll compare the result we get with sklearns linear regression implementation.\n",
        "\n",
        "I'm going to define some fake data first. For the X matrix, I will randomly sample two columns from the standard normal distribution as my input variables, and then I will add a column of ones, $x_0$, which will allow me to fit an intercept in my linear model. For the y variable, I am going to define y as $3x_1 - x_2 + 7 + \\epsilon$, where $x_1$ and $x_2$ are the two standard normal columns from my X matrix and $\\epsilon$ is the normally distributed error that a linear model is assumed to have. Thus, the optimal $\\beta$ values we should get are $\\beta_1, \\beta_2, \\beta_0 = 3, -1, 7$. Note that $\\beta_0$ is the intercept that we are learning, so we expect to get a value of 7.\n",
        "\n",
        "If that explanation wasn't clear, don't worry about it. The short version of that previous paragraph is that I defined fake data and we are going to do linear regression on it."
      ],
      "metadata": {
        "id": "74SnsdTsIv2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define some practice data\n",
        "np.random.seed(42)\n",
        "x1 = np.random.normal(size = 100)\n",
        "x2 = np.random.normal(size = 100)\n",
        "x0 = np.ones(100)\n",
        "X = np.vstack((x0, x1, x2)).T\n",
        "\n",
        "y = 3 * x1 - x2 + 7 + np.random.normal(scale = 0.5, size = 100) #note we have to add the 0 mean normally distributed noise due to the N and E assumptions of L.I.N.E."
      ],
      "metadata": {
        "id": "88Btf4-Jv8wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can do some linear regression. First let's learn the coefficients according to our formula from class: $\\beta^* = (X^TX)^{-1}X^Ty$. Observe that I calcluate the matrix inverse using numpy's implementation\n",
        "\n",
        "```\n",
        "np.linalg.inv\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "INiQxqshK22v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the optimal regression coefficients\n",
        "betas = np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "betas"
      ],
      "metadata": {
        "id": "e4iVRI6aK10-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should get values of beta that are close to [7, 3, -1] by definition of the model."
      ],
      "metadata": {
        "id": "Nlhud7LDLV3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compare with sklearn. Note since I included a column of 1's in my data, I initialize the sklearn model with fit_intercept = False. I've already set up the data to automatically fit an intercept.\n",
        "#If you don't add a column of 1's to your dataset, you won't automatically fit an intercept, so you should keep fit_intercept = True, which is the default option in sklearn\n",
        "lm = LinearRegression(fit_intercept = False).fit(X, y)\n",
        "\n",
        "#we can see the coefficients we learned with lm.coef_\n",
        "lm.coef_"
      ],
      "metadata": {
        "id": "F8zQDlmJLLDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like we get the same regression coefficients using either the formula from class or sklearn's implementation. That's a good sign!"
      ],
      "metadata": {
        "id": "1mCpnZ8kLzzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the model and the data\n",
        "plt.plot(X@betas, y, 'k.')\n",
        "plt.plot(X@betas, X@betas, 'r-')\n",
        "plt.xlabel('Regression Model Predictions')\n",
        "plt.ylabel('Actual Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "faKz9MgCLOMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing the Linearity assumption, and multicollinearity**\n",
        "\n",
        "Note, we went out of order here. You should test these things ***before*** you fit the model, but this is just a practice notebook, so we don't care about the order right now."
      ],
      "metadata": {
        "id": "Sc44xozWMtZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we can test for linearity and multicollinearity of the input variables at the same time using a seaborn pairplot and correlation matrix\n",
        "df = pd.DataFrame(X, columns = ['x0', 'x1', 'x2'])\n",
        "df['y'] = y\n",
        "sns.pairplot(data = df[['x1', 'x2', 'y']])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L0rdPw2kMIL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['x1', 'x2', 'y']].corr()"
      ],
      "metadata": {
        "id": "U3bHGN9MM4oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the plots, I would say that x1 and y are strongly correlated, so I would conclude that x1 should go in the model. I would conclude that x2 and y are moderately correlated, so x2 should go in the model as well. Finally, there doesn't appear to be strong correlation between x1 and x2, so I am not worried about multicollinearity.\n",
        "\n",
        "The correlation matrix backs up my conclusions from the plot. x1 and y are very strongly correlated with a correlation of 0.94. x2 and y are also moderately correlated with a correlation of -0.42. So both of those variables could justifiably be added to the linear regression model. Finally, the correlation between x1 and x2 is just -0.14, which is pretty week, so I'm not worried about multicollinearity."
      ],
      "metadata": {
        "id": "p6Oijpm8OHLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Danger ahead!!**\n",
        "\n",
        "In the next cell, I show the true problem with having multicollinearity in a linear regression model. I define a new X matrix with a third column that is a scalar multiple of $x_1$, meaning I have perfect multicollinearity and $X^TX$ is not invertible. Let's see what happens"
      ],
      "metadata": {
        "id": "DHHTE_DzPDzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the new column\n",
        "x3 = 2 * x1\n",
        "\n",
        "new_X = np.hstack((X, x3.reshape((-1,1))))\n",
        "\n",
        "#now what happens when I try to solve for the regression coefficients\n",
        "new_betas = np.linalg.inv(new_X.T @ new_X) @ new_X.T @ y"
      ],
      "metadata": {
        "id": "EYjUqdk4NmQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should have gotten an error \"Singular matrix\" when you ran the previous cell. That's because $X^TX$ is not invertible in that code cell because of the multicollinearity between $x_1$ and $x_3$. A singular matrix is just another way to say a non-invertible matrix.\n",
        "\n",
        "In the next cell, we see how sklearn's linear regression implementation deals with perfect multicollinearity"
      ],
      "metadata": {
        "id": "xYBw145_PpjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm = LinearRegression(fit_intercept = False).fit(new_X, y)\n",
        "\n",
        "#we can see the coefficients we learned with lm.coef_\n",
        "lm.coef_"
      ],
      "metadata": {
        "id": "gp_9C8fpPfIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why did that work? It shouldn't have right? Remember in class we learned that the problem with perfect multicollinearity is that there is no unique solution to minimize the least squares loss function. In the case of a non invertible matrix, there are multiple possible solutions. Rather than raising an error that says \"Hey dummy your data is multicollinear. You should use more data science feel!\" sklearn instead just chooses one of the possible solutions. In particular, it chooses the solution that has smallest two norm according to the documentation here https://numpy.org/doc/2.1/reference/generated/numpy.linalg.lstsq.html\n",
        "\n",
        "**That is why it is so important to have data science feel!** A bad data scientist will take a bunch of multicollinear input variables and feed it into sklearn's linear regression model and they think they have done something valuable and impressive because sklearn succesfully fits a model without raising any errors. We know better! We understand the problem of multicollinearity, so we will never make the mistake of feeding two highly correlated input variables into the same regression model!"
      ],
      "metadata": {
        "id": "Cs3nRiasQzRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.linalg.solve(new_X.T @ new_X, new_X.T @ y)"
      ],
      "metadata": {
        "id": "ObVTULThQE7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verifying Independence**\n",
        "\n",
        "The next letter in our L.I.N.E. acronym is I, which stands for independence. Unfortunately, I'm not sure of a good way to test for independence. I was taught to just use some feel to deside if you think the errors are independent.\n",
        "\n",
        "In the case of this problem, I defined the data by drawing independent samples from normal distributions and adding independently sampled noise from another normal distribution, so I know for a fact that the independence assumption has been met.\n",
        "\n",
        "Consider another famous dataset that we have seen in this class, the iris dataset, which is a dataset containing measurements of petals and sepals on different classes of irises. Is there any reason to think that the length of petals on one iris would effect the length of petals on another iris? Probably not, so we can probably assume independence. However, suppose that if one type of iris is planted near another, it somehow dominates the other one by taking all the soil nutrients or all the sunlight or something. That would mean that the measurements of the big dominate iris and the weak dominated iris are not independent of each other in that case, so the independence assumption doesn't hold. That's the type of thinking I would say you need to do when trying to verify the independence assumption."
      ],
      "metadata": {
        "id": "Fferi6NMSyqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normality Assumption**\n",
        "\n",
        "The next assumption we need to verify is are the residuals (errors) of our regression model normally distributed. Just like we learned when we learnd about EDA, a good way to asses the shape of a distribution is with some type of distribution plot like a histogram or KDE"
      ],
      "metadata": {
        "id": "XvhLA0qwUGhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get the residuals\n",
        "\n",
        "resid = y - X @ betas\n",
        "plt.hist(resid, bins = 10, density = True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAq_vCGxQi27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That looks fairly normal to me, so I would say we pass the normality assumption. IF I want to be more thorough I could use a statistical test like the shapiro-wilk test or the jarque-bera test (google those if you are curious), but for this practice notebook I think the above histogram is good enough."
      ],
      "metadata": {
        "id": "4sCdCOk5Ucb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Equal Variance Assumption**\n",
        "\n",
        "As we saw in the class slides, the best way that I know of to test equal variance is with a residual versus fitted values plot. What we want to see is that the points are randomly distributed around the center line and they don't get further or closer to the line as the fitted values increase. We can create those plots with seaborn, or we can do it manually"
      ],
      "metadata": {
        "id": "JO2KckuOU4Cu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#doing it manually\n",
        "plt.plot(X@betas, resid, 'k.')\n",
        "plt.axhline(y=0, color = 'red')\n",
        "plt.xlabel('Fitted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kJNFVUKCUZfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with seaborn\n",
        "sns.residplot(x = X@betas, y = df.y.values)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "giGP717DVck1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would make the claim that the above plot(s) support the equal variance assumption. Thus we have passed all the required L.I.N.E. assumptions and the multicollinearity assumption!"
      ],
      "metadata": {
        "id": "btTL1oxpWOyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bonus Useful Material**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3sFQK3MbWf-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dealing with categorical variables\n",
        "\n",
        "It is possible to incorporate categorical variables into a linear model but we need to convert them into numbers because we need to be able to multiply their values by slopes. The way to do that is by \"encoding\" them. There are lots of ways to encode categorical variables, but I recommend a method called one hot encoding. One hot encoding creates a new column for each category where 1 means the category is present and 0 means it isn't. For example, if I were to one hot encode a variable that describes if an animal is a cat or a dog, then I would end up with two new columns is_cat and is_dog. Note, you shouldn't include both of those columns in your training data though because if is_dog is 1 then we know is_cat is 0, so including both columns is unnecessary. In general when you one hot encode $n$ categories, you should only keep $n-1$ of the resulting columns in your training data.\n",
        "\n",
        "In the following cell, I show how you can one hot encode data in pandas. Specifically, I one hot encode a pitcher handedness column, which I frequently have to do at work."
      ],
      "metadata": {
        "id": "pLBZmPqfneyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fake_data = pd.DataFrame({'pitcher_id': [1,2,3,4,5,6,7,8,9],\n",
        "    'pitcher_throws': ['R', 'R', 'L', 'L', 'R', 'R', 'L', 'R', 'L']})\n",
        "\n",
        "#one hot encode the pitcher_throws column\n",
        "pit_hand = pd.get_dummies(fake_data['pitcher_throws'], drop_first = False, dtype = int)\n",
        "pit_hand"
      ],
      "metadata": {
        "id": "6bOLJlP0Vzqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now I just choose one of the one hot encoded columns to include in my final dataset, and I can go ahead and drop the unencoded column if I want to\n",
        "\n",
        "#add the encoded column to the dataset\n",
        "fake_data['pit_hand_R'] = pit_hand['R'].values\n",
        "fake_data"
      ],
      "metadata": {
        "id": "xhycbD6IoyeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop the unencoded column from the original data (optional)\n",
        "fake_data.drop(columns = ['pitcher_throws'], inplace = True)\n",
        "fake_data"
      ],
      "metadata": {
        "id": "7jYit8T8pfnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering nonlinearities\n",
        "\n",
        "In the following cell, I generate two very nonlinear datasets. In the first, I sample x values from the standard normal distribution then define y as $x^2 + \\epsilon$, where $\\epsilon$ is normally distribued error. In the second, I again sample x values from a uniform distribution then define y as $\\text{cos}(x) + \\epsilon$."
      ],
      "metadata": {
        "id": "_ZtJeO0ZpnQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset 1\n",
        "x = np.random.normal(size = 100)\n",
        "y = x**2 + np.random.normal(scale = 0.5, size = 100)\n",
        "\n",
        "#plot x and y with a linear regression curve\n",
        "sns.regplot(x= x, y = y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bOUUodaJpjMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly, the L assumption in L.I.N.E. is not met in this dataset, and the resulting linear regression model does not fit the data well. However, ***that does not mean that we can't/shouldn't fit a linear regression model!*** It just means that we need to feature engineer the nonlinearity. In the following cell, I again feature engineer a new input variable x2, which I define simply by squaring x. Then I fit the linear regression model to x2 and y instead of x and y."
      ],
      "metadata": {
        "id": "R1fWI-F3tnu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm =LinearRegression().fit(x.reshape((-1,1))**2, y.reshape((-1,1)))\n",
        "\n",
        "plt.plot(x, y, 'k.', label = 'data')\n",
        "xpoints = np.linspace(np.min(x), np.max(x))\n",
        "plt.plot(xpoints, lm.predict(xpoints.reshape((-1,1))**2), 'r-', label = 'regression curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x0VTuWp3tktt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous cell, I just performed polynomial regression, but it's really just linear regression with a feature engineered $x^2$ input variable. In the following cell I show that what I did was feature engineer a linear relationship. You can see that there is a strong linear relationship between x**2 and y."
      ],
      "metadata": {
        "id": "DKUgPOytuiCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x**2, y, 'k.')\n",
        "plt.xlabel('x**2')\n",
        "plt.ylabel('y')\n",
        "plt.show()\n",
        "print('correlation: ' + str(np.corrcoef(x**2, y)[0,1]))"
      ],
      "metadata": {
        "id": "LKLpzEXHw3mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I move on to the second dataset where y is defined as a function of cos(x)"
      ],
      "metadata": {
        "id": "ov1FAra7xEE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset 2\n",
        "x = np.random.uniform(-2*np.pi, 2*np.pi, size = 100)\n",
        "y = np.cos(x) + np.random.normal(scale = 0.3, size = 100)\n",
        "\n",
        "#plot x and y with a linear regression curve\n",
        "sns.regplot(x= x, y = y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q1WvUVrEuLgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, it is clear that our data does not pass the L test in the L.I.N.E. acronym, but again that does not mean we can't do linear regression, it just means we need to feature engineer the linear relationship. This time, I do that by defining a new variable which is the cosine of x"
      ],
      "metadata": {
        "id": "s4Qu_3Otu1yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm =LinearRegression().fit(np.cos(x).reshape((-1,1)), y.reshape((-1,1)))\n",
        "\n",
        "plt.plot(x, y, 'k.', label = 'data')\n",
        "xpoints = np.linspace(np.min(x), np.max(x))\n",
        "plt.plot(xpoints, lm.predict(np.cos(xpoints).reshape((-1,1))), 'r-', label = 'regression curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "shbxcSr0uyAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.cos(x), y, 'k.')\n",
        "plt.xlabel('cos(x)')\n",
        "plt.ylabel('y')\n",
        "plt.show()\n",
        "print('correlation: ' + str(np.corrcoef(np.cos(x), y)[0,1]))"
      ],
      "metadata": {
        "id": "p6f0ZavlvWQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Takeaway**\n",
        "\n",
        "The thing that I want you to take away from the previous two examples is that just because the L assumption is not initially met in your data does not mean that you can't or even that you shouldn't perform linear regression. It just means that you need to do some feature engineering to create a linear relationship. In the previous cell you can see that I feature engineered a really strong linear relationship by taking the cosine of x, and then I was able to learn a really good linear regression model.\n",
        "\n",
        "Some tips for feature engineering nonlinearities:\n",
        "\n",
        "\n",
        "*   If the scatter plot looks wavy, consider sine or cosine\n",
        "*   If the scatter plot seems to change direction n times, consider adding features $x^2, x^3, \\ldots, x^n, x^{n+1}$. In other words, engineer a polynomial with max degree 1 higher than the number of changes of direction. For example, a parabola changes direction 1 time, and we know that we can fit parabolas well with $x^2 = x^{1 + 1}$\n",
        "* If the scatter plot seems to plateau as x increases, consider using log(x) or sqrt(x), although be careful because neither of those functions can take negative values, so if your data is negative you need to think of something else\n",
        "* If the scatter plot decays, you can try exponential decay np.exp(-x)\n",
        "\n",
        "One of the downsides of linear modeling is that you have to manually do all this feature engineering, whereas decision trees and ensembles built with decision trees and neural networks automatically figure all this out for you. That's why despite their simplicity, linear modeling often requires the most effort and data science feel to get it right. It can be worth the extra effort though because linear models are more easily interpreted and explained than those other types of models.\n",
        "\n"
      ],
      "metadata": {
        "id": "f1YZXO39vKW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering Interactions\n",
        "\n",
        "The idea behind interaction terms is that the effect that one variable has on the output depends on another variable. For example, suppose that I have an anxiety medication, and when I compare its effectiveness between males and females, I see that the reduction in anxiety as a function of dosage level is greater for females than it is for men:\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "That suggests that I need an interaction term between my one hot encoded sex variable and the dosage variable because the effect that dosage has on anxiety depends on the sex of the patient."
      ],
      "metadata": {
        "id": "omt2sDnFxJji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The good news is that feature engineering interactions is extremely easy. All you have to do is create a new variable that is the product of the two variables of interest. So in the previous anxiety medication example, I would create a new variable that is the prodcut of dosage and is_male or is_female. If you have more questions about why that works, feel free to ask me, but think about it in the context of a linear regression model and you may gain some intuition on your own."
      ],
      "metadata": {
        "id": "sdszHn6py-92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, just like feature engineering nonlinearities, we have to feature engineer all necessary interactions in linear models but we don't have to in tree based models or neural networks. Those types of models figure out the interactions for us automatically. Again, that is why linear modeling requires more thoughtfulness and EDA and effort than other types of modeling, but it can often be worth that extra effort!"
      ],
      "metadata": {
        "id": "BOadC1K5zbiL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "81pbG98kwRXK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}