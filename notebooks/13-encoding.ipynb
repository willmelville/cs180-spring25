{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/rhodes-byu/cs180-winter25/blob/main/notebooks/13-encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.datasets import fetch_openml\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Continuous (Numeric) Featuers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization\n",
    "Standardization is the process of scaling features to have a mean of 0 and a standard deviation of 1. The formula for standardization is:\n",
    "\n",
    "$$ z = \\frac{x - \\mu}{\\sigma} $$\n",
    "\n",
    "where:\n",
    "- $z$ is the standardized value  \n",
    "- $x$ is the original value  \n",
    "- $\\mu$ is the mean of the feature  \n",
    "- $\\sigma$ is the standard deviation of the feature  \n",
    "\n",
    "#### Normalization\n",
    "Normalization is the process of scaling features to a range of [0, 1]. The formula for normalization is:\n",
    "\n",
    "$$ x' = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}} $$\n",
    "\n",
    "where:\n",
    "- $x'$ is the normalized value  \n",
    "- $x$ is the original value  \n",
    "- $x_{\\min}$ is the minimum value of the feature  \n",
    "- $x_{\\max}$ is the maximum value of the feature  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization vs. Standardization\n",
    "\n",
    "### **Use Normalization (Scaling to [0, 1] or [-1, 1]) When:**\n",
    "- **Bounded Data**: Features have a fixed range (e.g., pixel values [0, 255]).\n",
    "- **Deep Learning**: Neural networks perform better with small, scaled inputs.\n",
    "- **Distance-Based Models**: k-NN, K-Means, and clustering methods rely on consistent feature scales.\n",
    "- **Non-Gaussian Data**: Works even when data isn't normally distributed.\n",
    "- **Interpretability**: Easier to understand in real-world terms.\n",
    "\n",
    "### **Use Standardization (Zero Mean, Unit Variance) When:**\n",
    "- **Gaussian-Like Data**: Ideal for normally distributed features.\n",
    "- **Linear Models & PCA**: Regression, SVM, and PCA assume standardized inputs.\n",
    "- **Outlier Robustness**: Less sensitive to extreme values than normalization.\n",
    "- **Different Units**: Useful when features have varying scales (e.g., income vs. age).\n",
    "- **Optimization Stability**: Gradient-based models (SGD, Adam) converge better.\n",
    "\n",
    "### **Key Takeaways:**\n",
    "- **Normalization**: Best for bounded data, deep learning, and distance-based models.\n",
    "- **Standardization**: Best for Gaussian-like data, linear models, and handling different units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Scaling / Normalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(loc = 10, scale = 3, size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X), np.std(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Note: Sklearn requires at least one column; the reshape ensures a column vector\n",
    "X_scaled = scaler.fit_transform(X.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X_scaled), np.std(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = MinMaxScaler()\n",
    "\n",
    "X_normalized = normalizer.fit_transform(X.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(X_normalized), np.max(X_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn StandardScaler converts to array\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas apply to keep as dataframe; filter by float columns\n",
    "df_standardized = df.apply(lambda x: (x - x.mean()) / x.std() if x.dtype == 'float64' else x)\n",
    "df_standardized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas normalization\n",
    "df_normalized = df.apply(lambda x: (x - x.min()) / (x.max() - x.min()) if x.dtype == 'float64' else x)\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Categorical Features\n",
    "\n",
    "### Label Encoding\n",
    "\n",
    "Typically used to encode the labels or targets when labels are categories.  \n",
    "\n",
    "`LabelEncoder` from `sklearn.preprocessing` maps from categories (strings) to integer values.\n",
    "\n",
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "One-hot encoding splits up a single categorical feature (e.g., `['cat', 'dog', 'fish']`) into several columns which represent binary values, 1 mapped to the category of the observation, and 0 for the other categories.\n",
    "\n",
    "For example, the animal column with values `['cat', 'dog', 'fish', 'cat']` Would map to\n",
    "\n",
    "| cat | dog | fish |\n",
    "|-----|-----|------|\n",
    "|  1  |  0  |  0   |\n",
    "|  0  |  1  |  0   |\n",
    "|  0  |  0  |  1   |\n",
    "|  1  |  0  |  0   |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_str = ['zebra', 'dog', 'cat', 'fish', 'dog', 'cat', 'fish']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = OneHotEncoder(sparse_output = False)\n",
    "y_one_hot = one_hot_encoder.fit_transform(y_encoded.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the titanic dataset\n",
    "data = fetch_openml(data_id=40945, parser = 'auto')\n",
    "titanic = data.frame\n",
    "titanic.drop(['body', 'boat', 'name', 'ticket', 'home.dest', 'cabin'], axis = 1, inplace = True)\n",
    "titanic.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_encoded = titanic.apply(lambda x: pd.Categorical(x).codes if x.dtype == 'category' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_one_hot = pd.get_dummies(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_one_hot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **In-Class Activity: Predicting Obesity Levels from Eating Habits and Physical Condition**\n",
    "\n",
    "In this activity, you will work with a dataset designed to predict obesity levels based on various eating habits and physical conditions. Your goal is to preprocess the data, experiment with different encoding strategies, and compare classification models.\n",
    "\n",
    "---\n",
    "\n",
    "## **Review the Dataset**\n",
    "Before beginning, take some time to familiarize yourself with the dataset and its features. Feature descriptions can be found [here](https://archive.ics.uci.edu/dataset/544/estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition).\n",
    "\n",
    "Consider the following as you review the dataset:\n",
    "- What types of features are present? *(Numerical, ordinal, categorical?)*  \n",
    "- How should these features be encoded for use in machine learning models?\n",
    "\n",
    "---\n",
    "\n",
    "## **Data Preprocessing**\n",
    "- **Encoding:** Decide how to encode categorical and ordinal variables appropriately.\n",
    "- **Splitting:** Divide the dataset into **80% training** and **20% testing** using:\n",
    "\n",
    "\n",
    "## **Model Training & Cross-Validation**\n",
    "- Apply **cross-validation** on the training set to fine-tune hyperparameters and evaluate model performance.\n",
    "- Compare the results of **$k$-Nearest Neighbors (k-NN) and Logistic Regression** using cross-validation scores.\n",
    "\n",
    "### **Evaluation:**\n",
    "1. Compare the models based on accuracy.\n",
    "2. Consider hyperparameter tuning for both models:\n",
    "   - For **k-NN**, experiment with different values of k, metrics, and weighting.\n",
    "   - For **Logistic Regression**, consider trying different penalties. (View the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).)\n",
    "\n",
    "## **Wait Before Testing!**\n",
    "🚨 **Do NOT evaluate your model on the test set until instructed to do so!** 🚨  \n",
    "\n",
    "- The test set should remain **unseen** throughout training and validation.\n",
    "- We will use it **only once** to assess the final model’s performance.\n",
    "- Keep track of your cross-validation results to decide which model to use for final testing.\n",
    "\n",
    "### **Why is this important?**\n",
    "Evaluating too early on the test set can lead to **data leakage** and **overfitting**, giving misleading performance estimates. The test set should serve as a final, unbiased evaluation of the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the data:\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/rhodes-byu/cs180-winter25/refs/heads/main/data/obesity.csv')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
