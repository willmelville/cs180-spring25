{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/rhodes-byu/cs180-winter25/blob/main/notebooks/16-binary-metrics\n",
    ".ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Binary Confusion Matrix\n",
    "\n",
    "| Actual \\ Predicted | Positive (P) | Negative (N) |\n",
    "|---------------------|--------------|--------------|\n",
    "| Positive (P)        | True Positive (TP) | False Negative (FN) |\n",
    "| Negative (N)        | False Positive (FP) | True Negative (TN) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "source": [
    "### Recall (True Positive Rate, Sensitivity)\n",
    "\n",
    "Recall answers the question, \"**What proportion of the positives did we find?**\".   \n",
    "\n",
    "High recall score: the model finds all/most of the real positive cases, but potentially at the expense of more false positives as well.\n",
    "\n",
    "In cases where missing a positive prediction is bad (e.g., medical diagnosis), we want high recall. \n",
    "\n",
    "The formula for Recall is:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n",
    "$$\n",
    "\n",
    "(Note: The denominator is just the total number of positive values in the **original labels**.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Precision\n",
    "\n",
    "What proportion of all positive predictions are accurate? \n",
    "\n",
    "For example, we want high precision in cases such as a spam detector: we want to ensure all emails that are predicted to be spam are indeed spam. We don't want to lose important emails, but it is not that big of a deal of few spam emails get to our inbox.\n",
    "\n",
    "The formula for Precision is:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n",
    "$$\n",
    "\n",
    "(Note: The denominator is just the number of values that were **predicted** positive.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Recall / Precision Tradeoff:\n",
    "\n",
    "High recall, low precision: Finds most of the real positive cases, but at the expense of more false positives as well.\n",
    "\n",
    "High precision, low recall: All emails that are designated spam are indeed spam, but the model misses a lot of spam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The precision-recall curve (PR curve)\n",
    "\n",
    "The precision-recall curve (PR curve) is a graphical representation of the tradeoff between precision and recall for different threshold values of a classification model. It is particularly useful when dealing with imbalanced datasets, where one class significantly outnumbers the other.\n",
    "\n",
    "- **Precision** is plotted on the y-axis.\n",
    "- **Recall** is plotted on the x-axis.\n",
    "\n",
    "A good model will have a PR curve that bows towards the top-right corner, indicating high precision and recall across different thresholds.\n",
    "\n",
    "#### How the PR Curve is Generated:\n",
    "The PR curve is generated by varying the decision threshold of the classification model. For each threshold:\n",
    "- Predictions are classified as positive or negative based on whether their probability score exceeds the threshold.\n",
    "- Precision and recall are calculated based on the resulting confusion matrix.\n",
    "\n",
    "The area under the precision-recall curve (PR-AUC) is a single scalar value that summarizes the performance of the model. A higher PR-AUC indicates better performance. An AUC of 0.5 indicates no discriminative ability (equivalent to random guessing), while an AUC of 1.0 signifies perfect discrimination between classes.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://user-images.githubusercontent.com/26833433/76019078-0a79fb00-5ed6-11ea-8b5b-5697bbbd7e7e.png\" alt=\"PR-Curve Example\" width=\"400\"/>\n",
    "    <figcaption><a href=\"https://github.com/ultralytics/yolov3/issues/898\">https://github.com/ultralytics/yolov3/issues/898</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use the PR Curve\n",
    "\n",
    "- **Imbalanced Datasets**: The PR curve is more informative than the ROC curve when dealing with imbalanced datasets, where the positive class is rare. Examples include fraud detection and disease diagnosis.\n",
    "- **Costly False Positives**: If false positives are more significant or costly than false negatives (e.g., spam email detection), the PR curve is preferred as it emphasizes precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score\n",
    "\n",
    "The F1 score serves as a balance between precision and recall. It is the harmonic mean of precision and recall. The F1 score is calculated as follows:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "The F1 score ranges from 0 to 1, where 1 is the best possible F1 score. The F1 score balances the trade-off between these two metrics. A score of 1 indicates perfect precision (no false positives) and perfect recall (no false negatives), meaning the model made no classification errors. A score of 0 indicates either precision or recall is zero—meaning the model failed completely in identifying one of the classes correctly.\n",
    "\n",
    "In classification problems with imbalanced classes (e.g., fraud detection, rare disease diagnosis), accuracy can be misleading—predicting the majority class most of the time can yield high accuracy but poor performance on the minority class.\n",
    "The F1 score, by combining precision and recall, better reflects the model’s ability to correctly identify the minority class. It penalizes models that do well on only one of these two metrics, offering a more realistic view of model performance in such settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity and Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity\n",
    "\n",
    "Sensitivity is another term for Recall. (True positives out of all original positive labels.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specificity \n",
    "\n",
    "The proportion of true negatives out of the actual negatives.\n",
    "\n",
    "The formula for Specificity is:\n",
    "\n",
    "$$\n",
    "\\text{Specificity} = \\frac{\\text{True Negatives (TN)}}{\\text{True Negatives (TN)} + \\text{False Positives (FP)}}\n",
    "$$\n",
    "\n",
    "(Note: The denominator is just the total number of negative values in the **original labels**.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate (FPR)\n",
    "\n",
    "The False Positive Rate (FPR) is the ratio of negative instances that are incorrectly classified as positive. It complements the True Negative Rate (TNR), which measures the proportion of negatives correctly identified as such.\n",
    "\n",
    "$$\n",
    "\\text{FPR} = 1 - \\text{Specificity}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ROC Curve:\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation of a classifier's performance across different threshold values. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR).\n",
    "\n",
    "- **True Positive Rate (TPR)** is also known as Recall or Sensitivity.\n",
    "- **False Positive Rate (FPR)** is calculated as $ \\text{FPR} = 1 - \\text{Specificity} $.\n",
    "\n",
    "The area under the ROC curve (ROC-AUC) is a single scalar value that summarizes the model's performance. A higher ROC-AUC indicates better performance, with a value of 1 representing a perfect model and 0.5 representing a random classifier.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://www.mathworks.com/help/examples/nnet/win64/CompareDeepLearningModelsUsingROCCurvesExample_01.png\" alt=\"ROC Curve Example\" width=\"400\"/>\n",
    "    <figcaption>https://www.mathworks.com/help/deeplearning/ug/compare-deep-learning-models-using-ROC-curves.html</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use which curve?\n",
    "\n",
    "Use a ROC curve for balanced datasets to analyze the trade-off between true positive rate (TPR) and false positive rate (FPR), providing an overall measure of a model’s ability to discriminate between classes. However, it may not be ideal for imbalanced datasets as it considers true negatives.\n",
    "\n",
    "Use a PR curve for imbalanced datasets, especially when the positive class is rare or false positives are costly. Since it focuses on precision (positive predictive value) and recall (TPR), it better reflects model performance when class imbalance is a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, accuracy_score, roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, f1_score, confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import fetch_openml\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_openml(name=\"heart-disease\", version=1, as_frame=True, parser = 'auto')\n",
    "\n",
    "X = data.data\n",
    "y = X.pop('target')\n",
    "\n",
    "# data = load_breast_cancer()\n",
    "# X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "# y = pd.Series(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = X.apply(lambda x: (x - x.mean()) / x.std() if x.name in data.feature_names else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.3, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "# model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Recall (Of all actual positive, how many were predicted as positive)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Precision (Of all predicted positive, how many were actually positive)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(f'Precision: {precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### F1 Score (Harmonic mean of precision and recall)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decision Function and Predict Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For logistic regression, decision_function returns the logits\n",
    "# y_scores = model.decision_function(X_test)\n",
    "\n",
    "y_proba = model.predict_proba(X_test)\n",
    "y_scores = y_proba[:, 1]\n",
    "np.round(y_proba, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PR-Curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "print(len(thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thresholds)\n",
    "plt.title('Decision Boundary Thresholds')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Threshold Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=precisions, y=recalls)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    if i % 5 == 0:  # Add points for every 10th threshold to avoid clutter\n",
    "        plt.text(precisions[i], recalls[i] + 0.01, f'{threshold:.2f}', fontsize=8, color='red')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity and Specificity (by \"hand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "recall = tp / (tp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Recall: {recall}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'F1 Score: {f1}\\n')\n",
    "print(f'Sensitivity: {sensitivity}')\n",
    "print(f'Specificity: {specificity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_proba(proba, threshold=0.5):\n",
    "    proba = np.asarray(proba)  # Ensure numpy array\n",
    "\n",
    "    if proba.ndim == 1:  # Handle 1D case (implicitly binary)\n",
    "        y_scores = proba\n",
    "    elif proba.shape[1] == 2:\n",
    "        y_scores = proba[:, 1]  # Take probabilities of the positive class\n",
    "    elif proba.shape[1] == 1:\n",
    "        y_scores = proba.ravel()  # Flatten single-column array\n",
    "    else:\n",
    "        raise ValueError(\"Only binary cases are supported. proba must have shape (n, d) with d <= 2\")\n",
    "\n",
    "    y_pred = (y_scores >= threshold).astype(int)  # More efficient thresholding\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_thresh = predict_from_proba(y_proba, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "cm_thresh = confusion_matrix(y_test.values, y_pred_thresh)\n",
    "sns.heatmap(cm_thresh, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "plt.title('Confusion Matrix with Threshold')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(threshold=(0.0, 1.0, 0.01))\n",
    "def update(threshold=0.01):\n",
    "    y_pred_thresh = predict_from_proba(y_proba, threshold)\n",
    "    cm = confusion_matrix(y_test, y_pred_thresh)\n",
    "    ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('Actual Labels')\n",
    "    plt.title(f'Confusion Matrix at threshold = {threshold:.2f}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold_metrics(y_proba, y_true, threshold=0.5):\n",
    "    y_pred = predict_from_proba(y_proba, threshold)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1 = 2 * sensitivity * specificity / (sensitivity + specificity)\n",
    "\n",
    "    return sensitivity, specificity, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(threshold=(0.0, 1.0, 0.01))\n",
    "def plot_metrics(threshold=0.5):\n",
    "    sensitivity, specificity, f1 = get_threshold_metrics(y_proba, y_test, threshold)\n",
    "    sensitivity, specificity, f1 = np.round(sensitivity, 3), np.round(specificity, 3), np.round(f1, 3)\n",
    "\n",
    "    metrics = {'Sensitivity': sensitivity, 'Specificity': specificity, 'F1 Score': f1}\n",
    "    plt.bar(metrics.keys(), metrics.values(), color=['blue', 'green', 'orange'])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(f'Metrics at Threshold = {threshold:.2f}')\n",
    "    plt.ylabel('Score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "roc_auc = roc_auc_score(y_test, y_scores)\n",
    "\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='red')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(y_test, y_scores)\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rfgap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
