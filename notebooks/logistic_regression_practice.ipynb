{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUXrlXI-ciJO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from scipy.special import expit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Learning the Model with Synthetic Data**"
      ],
      "metadata": {
        "id": "6AiUJYCRhKdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like in the linear regression practice notebook, I'm going to generate some fake data. My X matrix will have two columns $x_1$ and $x_2$. I'm not adding a column of all ones to my data matrix this time, so when I train the model I need to remember to set ``fit_intercept = True``. Once again I will sample $x_1$ and $x_2$ from the standard normal distribution. Then I will define the log odds as $3x_1 - x_2 -1 + \\epsilon$ where $\\epsilon$ is the normally distributed error that linear models are assumed to have. Once I have the log odds defined, I use the sigmoid activation function to convert them to probabilities, then I use those probabilities to sample Bernoulli random variables (coin flips). The resulting Bernoulli samples will be my binary $y$ target variable."
      ],
      "metadata": {
        "id": "qlT9r4ttczhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the practice data\n",
        "np.random.seed(42)\n",
        "x1 = np.random.normal(size = 5000)\n",
        "x2 = np.random.normal(size = 5000)\n",
        "X = np.vstack((x1, x2)).T\n",
        "\n",
        "logits = 3 * x1 - x2 - 1 + np.random.normal(scale = 0.5, size = 5000)\n",
        "\n",
        "#the expit function from scipy.special is the sigmoid function\n",
        "probits = expit(logits)\n",
        "\n",
        "y = np.random.binomial(n = 1, p = probits)"
      ],
      "metadata": {
        "id": "Ouk0g455crTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're going to go out of order here since this is just a practice notebook and fit our logistic regression model first before testing the linearity and multicollinearity assumptions. In reality, you should always check those assumptions first. Nevertheless, let's learn a logistic regression model and see how close we get to the true coefficients of $\\beta_0, \\beta_1, \\beta_2 = -1, 3, -1$. Remember in this context, $\\beta_0$ is the intercept of the line."
      ],
      "metadata": {
        "id": "dtXv82HcekLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use sklearn to run logistic regression\n",
        "lr = LogisticRegression(fit_intercept = True, random_state = 24).fit(X, y)\n",
        "\n",
        "#we get the intercept beta_0 using lr.intercept_, and the coefficients beta_1 and beta_2 using lr.coef_\n",
        "lr.intercept_, lr.coef_"
      ],
      "metadata": {
        "id": "vnrc-FzaeLFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the learned log odds versus the true log odds. Normally we wouldn't be able to do this because there is no way to observe the true log odds given just binary target variables, but since we generated the data we get to see this plot! Similarly, we can plot the true probabilities, which we normally wouldn't know, with our model's outputted probabilities."
      ],
      "metadata": {
        "id": "TsPPz1Haffp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lr.intercept_ + X @ lr.coef_[0], logits, 'k.')\n",
        "plt.plot(lr.intercept_ + X @ lr.coef_[0] ,lr.intercept_ + X @ lr.coef_[0], 'r:')\n",
        "plt.xlabel('Predicted Log Odds')\n",
        "plt.ylabel('True Log Odds')\n",
        "plt.title('Log Odds Plot')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(lr.predict_proba(X)[:,1], probits, 'k.')\n",
        "plt.plot(lr.predict_proba(X)[:,1], lr.predict_proba(X)[:,1], 'r:')\n",
        "plt.xlabel('Predicted Probabilities')\n",
        "plt.ylabel('True Probabilities')\n",
        "plt.title('Probabilities Plot')\n",
        "plt.show()\n",
        "\n",
        "#finally, print the model accuracy\n",
        "acc = lr.score(X, y)\n",
        "print('Accuracy: ' + str(acc))"
      ],
      "metadata": {
        "id": "Q24wgQY4fTvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing the Linearity in Log Odds Assumption**\n",
        "\n",
        "Remember that Dr. Heaton taught me that since we can't actually observe the true log odds in our data, we test linearity in log odds by fitting a lowess curve to our binary data and then checking that we have monotonicity. He taught me how to do that in R using the scatter.smooth function. In the following cell I show how to do it in python."
      ],
      "metadata": {
        "id": "kITV_Ak_hEmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a lowess curve to compare x1 with y\n",
        "sns.regplot(x = x1, y = y, lowess = True)\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('y')\n",
        "plt.show()\n",
        "\n",
        "sns.regplot(x = x2, y = y, lowess = True)\n",
        "plt.xlabel('x2')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Li9ZOiRKgIjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When I say that we want monotonicity in these curves, I mean that they need to either always be decreasing or always be increasing. Both of the above plots pass that test, so both myself and Dr. Heaton would say that these plots suggest that the linearity in log odds assumption is met.\n",
        "\n",
        "In the following cell, I show an example where the monotonicity assumption would not be met."
      ],
      "metadata": {
        "id": "vMSUMzjuiLtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.regplot(x = x1**4, y = y, lowess = True)\n",
        "plt.xlabel('x1^4')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lN6j56G-iGE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The curve changes directions in the above plot, so there is not linearity in log odds between x1^4 and y."
      ],
      "metadata": {
        "id": "mYRVdUaEit0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus**: I like to say I invented the following function, although there is a good chance that someone else thought of this same idea before I did. We can't compare our input variables to the log odds to see if they have a linear relationship because our data doesn't have the log odds in it, it just has the binary class. However, if I group data points into bins, then I can calculate the pseudoprobability $p$ of $y = 1$ in each bin, which I can then use to calculate the log odds by taking log($\\frac{p}{1-p}$). So the idea of this function is to sort the data according to the inputted x values then group them into bins. Then I calculate the pseudo probability of each bin and the log odds, and then I can plot the average x value of each bin and the log odds to see if there is a linear relationship between x and the log odds. You can also plot the pseudoprobabilities along with the average x values by setting ``use_log_odds = False``. When you plot the probabilities, you hope to see an S-shaped curve because the sigmoid curve is S-shaped, so if you have true linearity in log odds, you will have an S-shape in probabilities.\n",
        "\n",
        "Keep in mind that I have only ever encountered one other data scientist who did something similar this, so you may not want to talk about this function in future job interviews, but I do think it is better than the above lowess curves for testing linearity, so once you get your data science jobs I'd encourage you to use this function when you train logistic regression models. Note this function will also work best with lots of data where you can have lots of bins with lots of points in each bin."
      ],
      "metadata": {
        "id": "QpXjnFIyi0nG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_log_odds(x, y, groupsize = 500, use_log_odds = True, title = '', xlab = '', ylab = ''):\n",
        "\n",
        "    #sort the data by x\n",
        "    sorted_x = x[np.argsort(x)]\n",
        "    sorted_y = y[np.argsort(x)]\n",
        "\n",
        "    predicted = []\n",
        "    actual = []\n",
        "    numgroups = len(x) // groupsize\n",
        "    for i in range(numgroups):\n",
        "        if i == numgroups-1:\n",
        "            rows = [j for j in range(i*groupsize, len(x))]\n",
        "        else:\n",
        "            rows = [j for j in range(i*groupsize, (i+1) * groupsize)]\n",
        "        group_x = sorted_x[rows]\n",
        "        group_y = sorted_y[rows]\n",
        "        actual.append(np.mean(group_y))\n",
        "        predicted.append(np.mean(group_x))\n",
        "    fig = plt.figure(figsize = (3.5,3.5))\n",
        "    if use_log_odds:\n",
        "        actual = np.array(actual)\n",
        "        plt.plot(predicted, np.log(actual / (1-actual)), 'k.')\n",
        "    else:\n",
        "        plt.plot(predicted, actual, 'k.')\n",
        "    plt.xlabel(xlab)\n",
        "    plt.title(title)\n",
        "    plt.ylabel(ylab)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Imr04UuRij6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now let's use that function to compare x1 and y\n",
        "plot_log_odds(x1, y, title = 'log odds plot', xlab = 'x1', ylab = 'log odds of y')\n",
        "plot_log_odds(x1, y, use_log_odds = False, title = 'probabilities plot', xlab = 'x1', ylab = 'probability of y')"
      ],
      "metadata": {
        "id": "Ur6ZAo8hlJQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These plots are what we want to see when testing for linearity. In the first plot, there is clearly a linear relationship between x1 and the log odds, and in the second plot where we plot the pseudoprobabilities we see a slight S-shape. Next let's do the same thing with x2"
      ],
      "metadata": {
        "id": "nxg_ivizmDMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_log_odds(x2, y, title = 'log odds plot', xlab = 'x2', ylab = 'log odds of y')\n",
        "plot_log_odds(x2, y, use_log_odds = False, title = 'probabilities plot', xlab = 'x2', ylab = 'probability of y')"
      ],
      "metadata": {
        "id": "tOyOJ9DUlT25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plots for x2 also look good. I would conclude that we have linearity in log odds from these plots.\n",
        "\n",
        "**My expectation** is that you will check linearity in log odds using the lowess method before doing logistic regression. You can also do it with my experimental method if you would like, but you have to ask yourself who do you trust more to guide you: Matthew Heaton or Will Melville??"
      ],
      "metadata": {
        "id": "C5GAa5CLmSsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Checking multicollinearity**\n",
        "\n",
        "Just like we did in the linear regression practice notebook, we check for multicollinearity using pair plots and correlation matrices. However, since I only have two input variables this time, I'll just plot them on one scatter plot and calculate the correlation between them."
      ],
      "metadata": {
        "id": "ENlOh3ZvmrDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x1, x2, 'k.')\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.show()\n",
        "\n",
        "print('correlation: ' + str(np.corrcoef(x1, x2)[0,1]))"
      ],
      "metadata": {
        "id": "xVjmUa6SmRog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no correlation between x1 and x2, so multicollinearity is not an issue."
      ],
      "metadata": {
        "id": "7G3xnWsTnEsP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YbRsw7zNnBKu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}