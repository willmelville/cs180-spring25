{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2yFa9qJDOSe"
   },
   "source": [
    "# Pandas\n",
    "\n",
    "The pandas module is one of the most powerful tools for data analysis.  Pandas was designed to work with tabular and heterogeneous data.  The original author of pandas is Wes McKinney, so it makes sense that most of his book \"Python for Data Analysis\" covers the functionality of pandas. In fact, chapters 5 - 11 are basically about what pandas can do.  \n",
    "\n",
    "Here are some of the things that I hope you can do by the end of the section:\n",
    "* Create Series and DataFrames (ch 5)\n",
    "* Index, slice, and filter (ch 5)\n",
    "* Examine your data (ch 5)\n",
    "* Compute summarization and descriptive statistics (ch 5)\n",
    "* Drop rows and columns (ch 5)\n",
    "* Create columns (ch 5)\n",
    "* Count the number of missing values (ch 7)\n",
    "* Drop or fill missing values (ch 7)\n",
    "* Drop duplicate rows (ch 7)\n",
    "* Combine categories of categorical data (ch 7)\n",
    "* Discretize numerical data (ch 7)\n",
    "* Have some practice with hierarchical indexing (ch 8)\n",
    "* Reset the index (ch 8)\n",
    "* Merge and concatenate DataFrames (ch 8)\n",
    "* Simple plots with pandas (ch 9)\n",
    "* Use .groupby() for category aggregation (ch 10)\n",
    "* Fill missing values by group summary statistics (ch 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ju1RU9WWDOSh"
   },
   "source": [
    "## Importing Pandas\n",
    "\n",
    "It is standard to use the alias ``pd`` when importing pandas.\n",
    "~~~\n",
    "import pandas as pd\n",
    "~~~\n",
    "I usually import numpy at the same time since pandas and numpy are often used in tandem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyYhJfptDOSh"
   },
   "outputs": [],
   "source": [
    "# Import Pandas library\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you can install pandas within the notebook:\n",
    "# !pip install pandas\n",
    "# OR\n",
    "# !conda install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mokmfDpDDOSi"
   },
   "outputs": [],
   "source": [
    "# Try:  Create a Series from a list\n",
    "x = [1,2,3,4,5]\n",
    "lab = ['a','b','c','d','e']\n",
    "\n",
    "s = pd.Series(x, index=lab)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MH-mNGXbDOSi"
   },
   "outputs": [],
   "source": [
    "# Creating a Series with a dictionary\n",
    "\n",
    "d = pd.Series({'a': 1, 'b': 2, 'c': 3})\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pA-jXpPuDOSj"
   },
   "source": [
    "## DataFrames\n",
    "DataFrames are the main data structure of pandas and were directly inspired by the R programming language.  DataFrames are a bunch of Series objects put together to share the same (row) index.  A DataFrame has both a row and a column index.  \n",
    "\n",
    "## Creating DataFrames\n",
    "DataFrames can also be created from lists, dictionaries, or numpy arrays.\n",
    "Syntax: pd.DataFrame(data=None, index=None, columns=None, dtype=None, copy=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1286pWSDOSj"
   },
   "outputs": [],
   "source": [
    "x = [[1, 2, 3],\n",
    "     ['a', 'b', 'c'],\n",
    "     [4, 5, 6]]\n",
    "\n",
    "x_df = pd.DataFrame(x, columns = ['p', 'd', 'q'], index = ['x', 'y', 'z'])\n",
    "print(x_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQca_SbjDOSj"
   },
   "outputs": [],
   "source": [
    "# Create a simple DataFrame\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "        'Age': [25, 30, 35],\n",
    "        'Salary': [50000, 60000, 75000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IhxW6qgDOSj"
   },
   "outputs": [],
   "source": [
    "# Accessing specific columns\n",
    "names = df['Name']\n",
    "ages = df['Age']\n",
    "\n",
    "# Accessing a specific row\n",
    "row = df.loc[1]\n",
    "\n",
    "# Accessing a specific element\n",
    "salary = df.at[2, 'Salary']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ud41ppC1DOSk"
   },
   "outputs": [],
   "source": [
    "# Display the results\n",
    "print(\"Names: \\n\", names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWD9PVCFDOSk"
   },
   "outputs": [],
   "source": [
    "print(\"Ages: \\n\", ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjtA2W6HDOSk"
   },
   "outputs": [],
   "source": [
    "print(\"Row 1: \\n\", row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3u6u9wLIDOSk"
   },
   "outputs": [],
   "source": [
    "print(\"Charlie's Salary:\", salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVDs2FswDOSk"
   },
   "outputs": [],
   "source": [
    "# Add a new column calculated from existing columns\n",
    "df['Birth Year'] = 2023 - df['Age']\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4arP7HjJDOSk"
   },
   "outputs": [],
   "source": [
    "# Sort the DataFrame by Age in descending order\n",
    "df_sorted = df.sort_values(by='Birth Year', ascending=True)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "df_sorted.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kigOHSSDOSk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcVzcAlpDOSk"
   },
   "source": [
    "## Read in some practice data\n",
    "\n",
    "pd.read_csv can be used to load in external .csv files  \n",
    "We can access a summary of the data using df.info()  \n",
    "We can use df.head() to view the first view entries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuLGuAZSDOSk"
   },
   "outputs": [],
   "source": [
    "## Iris data\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "iris = pd.read_csv(url, names=['sepal_length','sepal_width', 'petal_length', 'petal_width', 'class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4aDYI6hDOSk"
   },
   "source": [
    "## Looking at your DataFrame\n",
    "\n",
    "``df.head()``  \n",
    "``df.tail()``  \n",
    "``df.shape``  \n",
    "``df.info()``  \n",
    "``df.describe()``   \n",
    "``df.columns``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVPhPTQxDOSk"
   },
   "outputs": [],
   "source": [
    "iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6aRueORDOSl"
   },
   "outputs": [],
   "source": [
    "iris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKRHqQ61DOSl"
   },
   "outputs": [],
   "source": [
    "iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZ5TfVZxDOSl"
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "iris.rename(columns={'class': 'species'}, inplace=True)\n",
    "\n",
    "# Display the DataFrame with renamed columns\n",
    "iris.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQy9e7zCDOSl"
   },
   "source": [
    "## Basic Plotting\n",
    "Pandas can be used for basic plotting, but we will cover more later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRwEBbLbDOSl"
   },
   "outputs": [],
   "source": [
    "iris['sepal_length'].plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eE7FvE9HDOSl"
   },
   "outputs": [],
   "source": [
    "iris.plot.scatter('sepal_length','sepal_width', c='petal_width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSs6XgaoDOSl"
   },
   "outputs": [],
   "source": [
    "iris.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bxf2Bh-UDOSl"
   },
   "outputs": [],
   "source": [
    "iris.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4yFi_7DDOSl"
   },
   "source": [
    "---\n",
    "\n",
    "## Selection and Indexing\n",
    "\n",
    "There are various ways to get subsets of the data.  In the following ``df`` refers to a DataFrame.\n",
    "\n",
    "#### Selecting columns\n",
    "One column (producing a Series)\n",
    "~~~\n",
    "df['column_name']\n",
    "df.column_name\n",
    "~~~\n",
    "---\n",
    "\n",
    "Multiple columns (producing a DataFrame)\n",
    "~~~\n",
    "df[['column_name']] # this will produce a DataFrame\n",
    "df[['col1', 'col2', 'col3']]\n",
    "~~~\n",
    "---\n",
    "\n",
    "#### Selecting row and columns with ``loc`` and ``iloc``\n",
    "~~~\n",
    "df.loc['row_name', 'col_name']\n",
    "df.iloc['row index', 'col index']\n",
    "~~~\n",
    "\n",
    "``loc`` and ``iloc`` also support slicing.  Note: when slicing with ``loc``, the end point IS including (but not when slicing with ``iloc``.\n",
    "\n",
    "---\n",
    "~~~\n",
    "df.loc['row_name1':'row_name2', 'col_name1':'col_name2']\n",
    "df.loc[:, 'col_name1':'col_name2']\n",
    "df.loc['r1':'r2', :]\n",
    "df.loc[['r1','r2','r3'],['c1','c2]]\n",
    "~~~\n",
    "*When using `.loc()`, `row_name2` and `col_name2` WILL be included*\n",
    "\n",
    "---\n",
    "~~~\n",
    "df.iloc[index1:index2, col1:col2]\n",
    "~~~\n",
    "*When using `.iloc()`, `index2` and `col2` will NOT be included*\n",
    "\n",
    "---\n",
    "#### Selecting rows based on column condition\n",
    "~~~\n",
    "df[df[boolean condition]]\n",
    "\n",
    "df[mask]\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kt64D6uJDOSl"
   },
   "outputs": [],
   "source": [
    "iris.loc[0:5, ['petal_width', 'petal_length']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLXIabquDOSl"
   },
   "outputs": [],
   "source": [
    "iris.iloc[0:2, 0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZJ65SgDDOSl"
   },
   "outputs": [],
   "source": [
    "iris['sepal_length'] > 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Po_be23cDOSl"
   },
   "outputs": [],
   "source": [
    "## Slicing with a boolean series\n",
    "iris[iris['sepal_length'] > 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBwr3oqdDOSl"
   },
   "outputs": [],
   "source": [
    "# Filter data using multiple conditions (Note the parentheses!)\n",
    "filtered_iris = iris[(iris['sepal_length'] > 6) & (iris['petal_length'] > 5)]\n",
    "\n",
    "# Display the filtered data\n",
    "filtered_iris.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyaP7UG_DOSl"
   },
   "outputs": [],
   "source": [
    "# Reset to default 0,1...n index\n",
    "filtered_iris.reset_index(drop = True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Fj2UddtHN3X"
   },
   "source": [
    "## Multi-Index and Index Hierarchy\n",
    "\n",
    "Let us go over how to work with Multi-Index, first we'll create a quick example of what a Multi-Indexed DataFrame would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrNrvRq3HcTw"
   },
   "outputs": [],
   "source": [
    "# Index Levels\n",
    "outside = ['G1','G1','G1','G2','G2','G2']\n",
    "inside = [1,2,3,1,2,3]\n",
    "hier_index = list(zip(outside,inside))\n",
    "hier_index = pd.MultiIndex.from_tuples(hier_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqmaCVtMHeSj"
   },
   "outputs": [],
   "source": [
    "hier_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Mh7fEZyHe8D"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(6,2),index=hier_index,columns=['A','B'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaV88xiaHfhS"
   },
   "source": [
    "Now let's show how to index this! For index hierarchy we use df.loc[], if this was on the columns axis, you would just use normal bracket notation df[]. Calling one level of the index returns the sub-dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_7vEfmlHpdI"
   },
   "outputs": [],
   "source": [
    "df.loc['G1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0NfjoBWHpVs"
   },
   "outputs": [],
   "source": [
    "df.loc['G1'].loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5x72aaouHpLk"
   },
   "outputs": [],
   "source": [
    "df.index.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjAgJuq4HpAR"
   },
   "outputs": [],
   "source": [
    "df.index.names = ['Group','Num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juONH7waHozu"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UllmZaVrHf2a"
   },
   "outputs": [],
   "source": [
    "# The xs() method in pandas is used to extract a cross-section from a DataFrame or Series\n",
    "df.xs('G1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDR3GkehDOSl"
   },
   "source": [
    "## Methods for computing summary and descriptive statistics\n",
    "pandas objects have many reduction / summary statistics methods that extract a single value from the rows or columms of a DataFrame.  See Table 5-8 in *Python for Data Analysis* for a more complete list, but here are a few that are commonly used.\n",
    "\n",
    "`count`: number of non-NA values   \n",
    "`describe`: summary statistics for numerical columns   \n",
    "`min`, `max`: min and max values  \n",
    "`argmin`, `argmax`: index of min and max values (for Series only)   \n",
    "`idxmin`, `idxmax`: index or column name of min and max values  \n",
    "`sum`: sum of values  \n",
    "`cumsum` : cummulative sum\n",
    "`mean`: mean of values  \n",
    "`quantile`: quantile from 0 to 1 of values  \n",
    "`var`: (sample) variance of values  \n",
    "`std`: (sample) standard deviation of values  \n",
    "`df.corr()` and `df.cov()` will produce the correlation or covariance matrix.  Or two Series can be used to get the correlation (or covariance) with `Series1`.corr(`Series2`).\n",
    "\n",
    "Numpy functions can also be used: `np.corrcoef()`\n",
    "\n",
    "Most of these functions also take an `axis` argument which specifies whether to reduce over rows or columns: 0 for rows and 1 for columns.   \n",
    "There is also an argument `skipna` which specifies whether or not to skip missing values.  The default is True.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84wB2FoeDOSl"
   },
   "outputs": [],
   "source": [
    "iris.sepal_length.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y99_yvm5DOSp"
   },
   "outputs": [],
   "source": [
    "iris.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUMM3yHoDOSp"
   },
   "source": [
    "## Unique values and value counts\n",
    "\n",
    "``df.nunique()`` or ``df['column'].nunique()``  \n",
    "\n",
    "``df.value_counts()`` or ``df['column'].value_counts()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUTcSnT6DOSp"
   },
   "outputs": [],
   "source": [
    "iris.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4xbl1-ODOSp"
   },
   "outputs": [],
   "source": [
    "iris.species.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESR4rkimDOSp"
   },
   "source": [
    "`df.corr()` and `df.cov()` will produce the correlation or covariance matrix.  Or two Series can be used to get the correlation (or covariance) with `Series1`.corr(`Series2`).\n",
    "\n",
    "Numpy functions can also be used: `np.corrcoef()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roEKPoRSDOSp"
   },
   "outputs": [],
   "source": [
    "iris.corr(numeric_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsur0pX4DOSp"
   },
   "source": [
    "---\n",
    "## Dropping rows and columns\n",
    "\n",
    "Columns and rows can be dropped with the `.drop()` method (using `axis=1` for columns and `axis=0` (default) for rows).  This method creates a new object unless `.inplace = True` is specified.\n",
    "\n",
    "The `del` command can also be used to drop columns in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAK4DgoUDOSp"
   },
   "outputs": [],
   "source": [
    "no_species = iris.drop('species', axis = 1)\n",
    "no_species.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHMeU8FJDOSp"
   },
   "outputs": [],
   "source": [
    "# The original is unchanged if inplace = False\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qsRj2gRDOSp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0pjnWMmDOSp"
   },
   "source": [
    "## Adding columns\n",
    "\n",
    "Add a new column to the end of a data frame\n",
    "~~~\n",
    "df['new_col'] = value\n",
    "~~~\n",
    "\n",
    "Add a new column at a specific index\n",
    "\n",
    "`.insert(col_index, 'new_col_name', value(s))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUvFmVgaDOSp"
   },
   "outputs": [],
   "source": [
    "iris['sum_petal_dims'] = iris['petal_length'] + iris['petal_width']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tL1lm4BoDOSp"
   },
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJHMQ9bqDOSq"
   },
   "source": [
    "## Using Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9R4Z4-TVDOSq"
   },
   "outputs": [],
   "source": [
    "iris['sepal_length'].apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bNrDTAXDOSq"
   },
   "outputs": [],
   "source": [
    "# What happened?\n",
    "iris['sepal_length'].apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TGrpObFDOSq"
   },
   "outputs": [],
   "source": [
    "iris.iloc[:, 0:4].apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCSdNVLHDOSq"
   },
   "outputs": [],
   "source": [
    "iris['species'].apply(lambda x: x.title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4-TGxHFDOSq"
   },
   "outputs": [],
   "source": [
    "iris['species'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVGP50VIDOSq"
   },
   "outputs": [],
   "source": [
    "def zero_one_scale(x):\n",
    "    return (x - np.min(x)) / (np.max(x)- np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Why does this not work?\n",
    "iris['sepal_length'].apply(zero_one_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Why does this not work?\n",
    "iris['petal_length'].apply(lambda x: zero_one_scale(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITZN99CpDOSq"
   },
   "outputs": [],
   "source": [
    "zero_one_scale(iris.petal_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j70DQ1UeDOSq"
   },
   "source": [
    "## Missing Values\n",
    "\n",
    "**Ways to count missing values**\n",
    "~~~\n",
    "df.info()\n",
    "df.isna().sum()\n",
    "df.isna().sum(axis=0)\n",
    "~~~\n",
    "\n",
    "**Drop missing values with `.dropna()`**\n",
    "\n",
    "Calling `.dropna()` without any arguments will drop all rows with missing values\n",
    "\n",
    "Arguments:\n",
    "* `axis=1` will drop columns with missing values (default is `axis=0`)\n",
    "* `how='all'` will drop rows (or columns) if all the values are NA (default is `how='any'`)\n",
    "* `subset=` will limit na search to these specic columns (or indexes)\n",
    "    \n",
    "\n",
    "**Fill missing values with `.fillna()`**\n",
    "Arguments:\n",
    "* `value`: value used to fill.\n",
    "* `method'`: methods used to fill (forward or backward fill)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITwS7RGkDOSq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtJJpcCzDOSq"
   },
   "outputs": [],
   "source": [
    "# Creating a DataFrame with missing values\n",
    "missing_data = {'A': [1, 2, np.nan],\n",
    "        'B': [np.nan, 4, 6],\n",
    "        'C': [7, 8, 9]}\n",
    "\n",
    "m_df = pd.DataFrame(missing_data)\n",
    "m_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHY4b6NHDOSq"
   },
   "outputs": [],
   "source": [
    "m_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GU9Q9kgyDOSq"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "m_df.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCyT7G1ZDOSq"
   },
   "outputs": [],
   "source": [
    "# Check how many missing values\n",
    "m_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v27TzUpgDOSq"
   },
   "outputs": [],
   "source": [
    "# Check how many missing values\n",
    "m_df.isna().sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HCStCbgDOSq"
   },
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "df_filled = m_df.fillna(-1)\n",
    "df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIGcsTZsDOSr"
   },
   "outputs": [],
   "source": [
    "m_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eankcnysDOSr"
   },
   "outputs": [],
   "source": [
    "# Fill with mean column value\n",
    "m_df.fillna(m_df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F79glDWIDOSr"
   },
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "df_dropped = m_df.dropna()\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "df_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fk6R9FHnDOSr"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNXp4rwnDOSr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1eejwalDOSr"
   },
   "source": [
    "## Groupby, Aggregation\n",
    "\n",
    "### Use Titanic data example here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "Hv9TS_UaDOSr"
   },
   "outputs": [],
   "source": [
    "## Titanic data\n",
    "# from sklearn.datasets import fetch_openml\n",
    "# dat = fetch_openml(data_id=40945, parser = 'auto')\n",
    "# titanic = dat.frame\n",
    "\n",
    "titanic = pd.read_csv('https://raw.githubusercontent.com/rhodes-byu/cs180-winter25/refs/heads/main/data/titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tl34baO9DOSr"
   },
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SMlggWJDOSr"
   },
   "outputs": [],
   "source": [
    "titanic.drop('name', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTvRRlnvDOSr"
   },
   "outputs": [],
   "source": [
    "# Average age by sex\n",
    "age_by_sex = titanic.groupby('sex')['age'].mean()\n",
    "\n",
    "# Display the aggregated data\n",
    "print(\"Age By Sex:\\n\", age_by_sex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyEi4Hw0DOSr"
   },
   "outputs": [],
   "source": [
    "# Multiple Grouping Categories\n",
    "titanic.groupby(['sex', 'pclass'])['age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMWIodYrDOSr"
   },
   "outputs": [],
   "source": [
    "# Multiple Target Variables\n",
    "titanic.groupby(['sex'])[['age', 'fare']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_U7b4rMMDOSr"
   },
   "outputs": [],
   "source": [
    "# Multiple Aggregations\n",
    "titanic.groupby('sex')['age'].agg(['mean', 'max', 'min', 'sum']).round()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9Lr7yTbDOSr"
   },
   "outputs": [],
   "source": [
    "# Define a custom aggregation function\n",
    "def range(series):\n",
    "    return series.max() - series.min()\n",
    "\n",
    "titanic.groupby('sex')['fare'].agg(range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "541_jlcGDOSr"
   },
   "outputs": [],
   "source": [
    "# Group data by 'Region' and apply named aggregations to multiple columns\n",
    "region_summary = titanic.groupby('home.dest').agg(\n",
    "    total_fare=('fare', 'sum'),\n",
    "    agerage_fare=('fare', 'mean'),\n",
    "    average_age=('age', 'mean')\n",
    ")\n",
    "\n",
    "# Display the summary for each region\n",
    "print(\"Region-wise Summary:\\n\", region_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cL8GvVyeDOSs"
   },
   "source": [
    "### Combining DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJnhwnIPDOSs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create two DataFrames\n",
    "df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\n",
    "                    'B': ['B0', 'B1', 'B2']})\n",
    "\n",
    "df2 = pd.DataFrame({'A': ['A3', 'A4', 'A5'],\n",
    "                    'B': ['B3', 'B4', 'B5']})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate DataFrames vertically\n",
    "result = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "# Display the concatenated DataFrame\n",
    "print(\"Concatenated DataFrame:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yB4TMeu4DOSs"
   },
   "outputs": [],
   "source": [
    "# Create two DataFrames with a common column 'key'\n",
    "left = pd.DataFrame({'key': ['A', 'B', 'C'],\n",
    "                     'value_left': [1, 2, 3]})\n",
    "\n",
    "right = pd.DataFrame({'key': ['B', 'C', 'D'],\n",
    "                      'value_right': [4, 5, 6]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames based on the 'key' column\n",
    "merged_inner = pd.merge(left, right, on='key', how='inner')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(\"Inner Merge:\\n\", merged_inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9oqj6QadDOSs"
   },
   "outputs": [],
   "source": [
    "merged_outer = pd.merge(left, right, on='key', how='outer')\n",
    "print(\"Outer Merge:\\n\", merged_outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5ib4cMZDOSs"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with wide-format data\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "        'Math_Score': [90, 85, 78],\n",
    "        'Science_Score': [88, 92, 80]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIUw0Aw4DOSs"
   },
   "outputs": [],
   "source": [
    "# Melt the DataFrame to long-format\n",
    "melted_df = pd.melt(df, id_vars=['Name'], var_name='Subject', value_name='Score')\n",
    "\n",
    "# Display the melted DataFrame\n",
    "print(\"Melted DataFrame:\\n\", melted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gl60QRePDOSs"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define a mapping function to assign letter grades\n",
    "def assign_grade(score):\n",
    "    if score >= 90:\n",
    "        return 'A'\n",
    "    elif score >= 80:\n",
    "        return 'B'\n",
    "    elif score >= 70:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'F'\n",
    "\n",
    "# Apply the mapping function to create a new column 'Grade'\n",
    "melted_df['Grade'] = melted_df['Score'].map(assign_grade)\n",
    "\n",
    "# Display the DataFrame with letter grades\n",
    "print(melted_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mLInKOxDOSs"
   },
   "outputs": [],
   "source": [
    "data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Scott', 'Liz'],\n",
    "        'Age': [28, 45, 60, 34, 50, 40]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define bin edges and labels for age groups\n",
    "bin_edges = [0, 30, 40, 50, 100]\n",
    "bin_labels = ['0-30', '31-40', '41-50', '51+']\n",
    "\n",
    "# Use the `cut` function to create a new column 'AgeGroup'\n",
    "df['AgeGroup'] = pd.cut(df['Age'], bins=bin_edges, labels=bin_labels)\n",
    "\n",
    "# Display the DataFrame with age groups\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKPVYsqdJ9tA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CylNK4RKJ9tA"
   },
   "outputs": [],
   "source": [
    "# Make the data directory if it doesn't exist\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to a CSV file\n",
    "df.to_csv('data/data.csv', index=False)\n",
    "\n",
    "# Read data from a CSV file\n",
    "new_df = pd.read_csv('data/data.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(new_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSAQVNW-J9tB"
   },
   "source": [
    "### Read in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7krctQi6J9tB"
   },
   "outputs": [],
   "source": [
    "# Downloading data to data/ directory (May not work on Windows)\n",
    "!curl -L -o data/example_csv.csv https://raw.githubusercontent.com/rhodes-byu/stat386-datasets/refs/heads/main/reading_examples/example_csv.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdhbVSZGJ9tB"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/example_csv.csv', index_col = 0, thousands = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rro4u6oPJ9tB"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtVGxKvVJ9tB"
   },
   "source": [
    "## read_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JqXvBWCJ9tC"
   },
   "outputs": [],
   "source": [
    "!curl -L -o data/example_excel.xlsx https://raw.githubusercontent.com/rhodes-byu/stat386-datasets/refs/heads/main/reading_examples/example_excel.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZAwv_SWJ9tC"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('data/example_excel.xlsx', sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kb9zOBXqJ9tC"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJGlus8sJ9tC"
   },
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaVmkQrDJ9tC"
   },
   "outputs": [],
   "source": [
    "lines = df['lines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-2gpvyKJ9tC"
   },
   "outputs": [],
   "source": [
    "lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kd8a1iqKJ9tC"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('data/example_excel.xlsx', sheet_name = 'lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mto-J9-xJ9tC"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSbf52uAJ9tC"
   },
   "source": [
    "## json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VpRqf0SJ9tC"
   },
   "outputs": [],
   "source": [
    "!curl -L -o data/example_json.json https://raw.githubusercontent.com/rhodes-byu/stat386-datasets/refs/heads/main/reading_examples/example_json.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Md9d8S3JJ9tC"
   },
   "outputs": [],
   "source": [
    "pd.read_json('data/example_json.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzytMS8TJ9tC"
   },
   "outputs": [],
   "source": [
    "with open('data/example_json.json', 'r') as file:\n",
    "    json_object = json.load(open('data/example_json.json', 'r'))\n",
    "\n",
    "print(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCehVkh-J9tD"
   },
   "outputs": [],
   "source": [
    "json_object['cap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMq4T45KJ9tD"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuJNsM8wJ9tD"
   },
   "outputs": [],
   "source": [
    "pd.json_normalize(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYnnG1I3J9tD"
   },
   "source": [
    "## read_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yd25hAu0J9tD"
   },
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_Super_Bowl_champions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3cjXcOPJ9tD"
   },
   "outputs": [],
   "source": [
    "# Reads in a list of DataFrames from the URL (based on tables)\n",
    "dfs = pd.read_html(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrcgXcaFJ9tD"
   },
   "outputs": [],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrJPkoIwJ9tD"
   },
   "outputs": [],
   "source": [
    "dfs[9].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-ZuchSIJ9tD"
   },
   "outputs": [],
   "source": [
    "pd.read_html(url, match = 'Joe Robbie')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcrF4tJBJ9tD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVlyaZW9J9tD"
   },
   "source": [
    "## Getting multiple pieces of information from a single column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEWLC0S0J9tD"
   },
   "source": [
    "### Unpacking\n",
    "\n",
    "Many times, a single column will contain multiple pieces of information.  Learning how to extract this information is extremely important and is a great skill to have.\n",
    "\n",
    "If it is possibe to somehow separate or split the elements in the column, this is a much easier and more effecive way of extracting information than simply extracting info based on slicing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VO3CV3QrJ9tE"
   },
   "source": [
    "For example, suppose we have a list of cities with the state.  We want to separate the city and the state into individual columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZxuCL_9J9tE"
   },
   "outputs": [],
   "source": [
    "cities = pd.Series(['Provo, Utah', 'Omaha, Nebraska', 'Fremont, Ohio','Green River, Wyoming', 'Durham, North Carolina' ])\n",
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9GFGUXgJ9tE"
   },
   "outputs": [],
   "source": [
    "for name in cities:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWY73wqzJ9tE"
   },
   "source": [
    "This looks like a hard problem because there are different lengths for each city and state name.  Some of the city names and state names even have spaces.  We recognize that there is a common format.  The city names are all separated from the state name by a comma.  We can use the string method ``.split(\"character\")`` to separate the words in a string based on ``\"character\"``.  \n",
    "\n",
    "By default, ``.split()`` will separate on spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74kIekyVJ9tE"
   },
   "outputs": [],
   "source": [
    "s = 'Provo, Utah'\n",
    "s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0tQsE0GJ9tE"
   },
   "outputs": [],
   "source": [
    "s.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjJYfDduJ9tE"
   },
   "outputs": [],
   "source": [
    "cities.apply(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyL-2sYNJ9tE"
   },
   "source": [
    "Or we could use  ``.str`` with ``.split``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeLRzxBEJ9tE"
   },
   "outputs": [],
   "source": [
    "cities.str.split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbSVjAuHJ9tE"
   },
   "source": [
    "Now we have a list of lists.  Next we need the get the information out.  We know that our Series had only one comma and when we split on the comma (using ``.split(\",\")``) everything before the comma is the first item in the list and everything after the comma is the second item in the list.  \n",
    "In our example, the first item is the city name and the second item is the state name.\n",
    "\n",
    "Here are a couple of ways to extract the data that was split.\n",
    "\n",
    "**First using a ``for`` loop:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ue3STQuEJ9tE"
   },
   "source": [
    "Notice that the state variable has white space, so we can strip that inside our for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NaSeGqAJ9tE"
   },
   "outputs": [],
   "source": [
    "# for loop\n",
    "cities_split = cities.str.split(\",\")\n",
    "\n",
    "state = []\n",
    "city = []\n",
    "for item in cities_split:\n",
    "    city.append(item[0].strip())\n",
    "    state.append(item[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOK4wsLlJ9tE"
   },
   "outputs": [],
   "source": [
    "cities_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDyctHX8J9tF"
   },
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5G44Ue1aJ9tF"
   },
   "outputs": [],
   "source": [
    "city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cvTy1HbJ9tF"
   },
   "source": [
    "**Second using ``.apply`` and ``lambda`` functions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idHAgVamJ9tF"
   },
   "outputs": [],
   "source": [
    "# apply with lambda function\n",
    "city = cities_split.apply(lambda x:x[0].strip())\n",
    "state = cities_split.apply(lambda x:x[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8zrZt1SJ9tF"
   },
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XDPRcDyJ9tF"
   },
   "outputs": [],
   "source": [
    "city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAZkkA6BJ9tF"
   },
   "source": [
    "**Another Example**\n",
    "\n",
    "Here, suppose I have times in the format ``hour:minute:second``.  I want to make a variable that combines these into just one time.  Since the lowest resolution is seconds, I will make a variable for \"seconds\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIafFq4-J9tF"
   },
   "outputs": [],
   "source": [
    "times = pd.Series(['01:34:07','00:35:12','00:00:16','03:59:00'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4w1O8zFJ9tF"
   },
   "outputs": [],
   "source": [
    "time_list = times.str.split(\":\")\n",
    "time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUaPSIixJ9tF"
   },
   "outputs": [],
   "source": [
    "seconds = []\n",
    "for time in time_list:\n",
    "    temp = int(time[0])*60*60 + int(time[1])*60 + int(time[2])\n",
    "    seconds.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4zhQUORJ9tF"
   },
   "outputs": [],
   "source": [
    "seconds"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "K4aDYI6hDOSk",
    "WQy9e7zCDOSl",
    "rDR3GkehDOSl"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
