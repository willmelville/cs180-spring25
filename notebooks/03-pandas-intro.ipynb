{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2yFa9qJDOSe"
      },
      "source": [
        "# Pandas\n",
        "\n",
        "The pandas module is one of the most powerful tools for data analysis.  Pandas was designed to work with tabular and heterogeneous data.  The original author of pandas is Wes McKinney, so it makes sense that most of his book \"Python for Data Analysis\" covers the functionality of pandas. In fact, chapters 5 - 11 are basically about what pandas can do.  \n",
        "\n",
        "Here are some of the things that I hope you can do by the end of the section:\n",
        "* Create Series and DataFrames (ch 5)\n",
        "* Index, slice, and filter (ch 5)\n",
        "* Examine your data (ch 5)\n",
        "* Compute summarization and descriptive statistics (ch 5)\n",
        "* Drop rows and columns (ch 5)\n",
        "* Create columns (ch 5)\n",
        "* Count the number of missing values (ch 7)\n",
        "* Drop or fill missing values (ch 7)\n",
        "* Drop duplicate rows (ch 7)\n",
        "* Combine categories of categorical data (ch 7)\n",
        "* Discretize numerical data (ch 7)\n",
        "* Have some practice with hierarchical indexing (ch 8)\n",
        "* Reset the index (ch 8)\n",
        "* Merge and concatenate DataFrames (ch 8)\n",
        "* Simple plots with pandas (ch 9)\n",
        "* Use .groupby() for category aggregation (ch 10)\n",
        "* Fill missing values by group summary statistics (ch 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju1RU9WWDOSh"
      },
      "source": [
        "## Importing Pandas\n",
        "\n",
        "It is standard to use the alias ``pd`` when importing pandas.\n",
        "~~~\n",
        "import pandas as pd\n",
        "~~~\n",
        "I usually import numpy at the same time since pandas and numpy are often used in tandem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyYhJfptDOSh"
      },
      "outputs": [],
      "source": [
        "# Import Pandas library\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7pvRLI9Kpyj"
      },
      "outputs": [],
      "source": [
        "# Note: you can install pandas within the notebook:\n",
        "# !pip install pandas\n",
        "# OR\n",
        "# !conda install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mokmfDpDDOSi"
      },
      "outputs": [],
      "source": [
        "# Try:  Create a Series from a list\n",
        "x = [1,2,3,4,5]\n",
        "lab = ['a','b','c','d','e']\n",
        "\n",
        "s = pd.Series(x, index=lab)\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH-mNGXbDOSi"
      },
      "outputs": [],
      "source": [
        "# Creating a Series with a dictionary\n",
        "\n",
        "d = pd.Series({'a': 1, 'b': 2, 'c': 3})\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you don't have to give an index. If you don't, it'll automatically index using integers starting at 0\n",
        "x = np.array([1,2,3,4,5])\n",
        "s = pd.Series(x)\n",
        "print(s)"
      ],
      "metadata": {
        "id": "oZpSGgo2K6QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA-jXpPuDOSj"
      },
      "source": [
        "## DataFrames\n",
        "DataFrames are the main data structure of pandas and were directly inspired by the R programming language.  DataFrames are a bunch of Series objects put together to share the same (row) index.  A DataFrame has both a row and a column index.  \n",
        "\n",
        "## Creating DataFrames\n",
        "DataFrames can also be created from lists, dictionaries, or numpy arrays.\n",
        "Syntax: pd.DataFrame(data=None, index=None, columns=None, dtype=None, copy=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1286pWSDOSj"
      },
      "outputs": [],
      "source": [
        "x = [[1, 2, 3],\n",
        "     ['a', 'b', 'c'],\n",
        "     [4, 5, 6]]\n",
        "\n",
        "x_df = pd.DataFrame(x, columns = ['p', 'd', 'q'], index = ['x', 'y', 'z'])\n",
        "print(x_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#again, you don't have to define an index. If you don't it'll do it automatically starting at 0\n",
        "x_df = pd.DataFrame(x, columns = ['p', 'd', 'q'])\n",
        "print(x_df)"
      ],
      "metadata": {
        "id": "giwQUHQ5Leh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#you also don't have to define the column names. . .\n",
        "x_df = pd.DataFrame(x)\n",
        "print(x_df)"
      ],
      "metadata": {
        "id": "mrA402bpLm_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#. . . and you can define columns and indices later\n",
        "x_df.columns = ['p', 'd', 'q']\n",
        "x_df.index = ['x', 'y', 'z']\n",
        "print(x_df)"
      ],
      "metadata": {
        "id": "QhqCCECRLr0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQca_SbjDOSj"
      },
      "outputs": [],
      "source": [
        "# Create a simple DataFrame with a dictionary. When you use a dictionary, the columns are automatically named using the keys in the dictionary\n",
        "data = {'Name': ['Alice', 'Bob', 'Charlie'],\n",
        "        'Age': [25, 30, 35],\n",
        "        'Salary': [50000, 60000, 75000]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IhxW6qgDOSj"
      },
      "outputs": [],
      "source": [
        "# Accessing specific columns\n",
        "names = df['Name']\n",
        "ages = df['Age']\n",
        "\n",
        "# Accessing a specific row\n",
        "row = df.loc[1]\n",
        "\n",
        "# Accessing a specific element\n",
        "salary = df.at[2, 'Salary']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the results\n",
        "print(\"Names: \\n\", names)"
      ],
      "metadata": {
        "id": "EGZXqV95O7o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Ages: \\n\", ages)"
      ],
      "metadata": {
        "id": "g1kCMbynO9Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Row 1: \\n\", row)"
      ],
      "metadata": {
        "id": "F2O90MFlO_j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Charlie's Salary:\", salary)"
      ],
      "metadata": {
        "id": "hAqIGMrdPBZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I had forgotten about '.at' before I reviewed this notebook. I have always used loc which can do the same thing and we'll cover in subsequent cells.\n",
        "df.loc[2, 'Salary']"
      ],
      "metadata": {
        "id": "Il9dKFCcNhjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#one advantage of loc over at is that you can access multiple values. For example, say I want Bob and Alice's salaries:\n",
        "df.loc[df['Name'].isin(['Bob', 'Alice']), 'Salary']"
      ],
      "metadata": {
        "id": "B0r-ty3eONVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#at cannot do that. It can only do single values, meaning one row index and one column index.\n",
        "df.at[df['Name'].isin(['Bob', 'Alice']), 'Salary']"
      ],
      "metadata": {
        "id": "6YFTcQQLOahn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**: difference between loc and at is that at can only get a single value, so it requires a single row and column index. loc can get multiple values, mutliple rows and/or multiple columns."
      ],
      "metadata": {
        "id": "RlgNdE79OmHr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVDs2FswDOSk"
      },
      "outputs": [],
      "source": [
        "# Add a new column calculated from existing columns\n",
        "df['Birth Year'] = 2023 - df['Age']\n",
        "\n",
        "# Display the DataFrame with the new column\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4arP7HjJDOSk"
      },
      "outputs": [],
      "source": [
        "# Sort the DataFrame by Age in descending order\n",
        "df_sorted = df.sort_values(by='Birth Year', ascending=True)\n",
        "\n",
        "# Display the sorted DataFrame\n",
        "df_sorted.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kigOHSSDOSk"
      },
      "outputs": [],
      "source": [
        "#you can also get the columns using .colname instead of ['colname']\n",
        "df_sorted.Salary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcVzcAlpDOSk"
      },
      "source": [
        "## Read in some practice data\n",
        "\n",
        "pd.read_csv can be used to load in external .csv files  \n",
        "We can access a summary of the data using df.info()  \n",
        "We can use df.head() to view the first view entries  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuLGuAZSDOSk"
      },
      "outputs": [],
      "source": [
        "## Iris data\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "iris = pd.read_csv(url, names=['sepal_length','sepal_width', 'petal_length', 'petal_width', 'class'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4aDYI6hDOSk"
      },
      "source": [
        "## Looking at your DataFrame\n",
        "\n",
        "``df.head()``  \n",
        "``df.tail()``  \n",
        "``df.shape``  \n",
        "``df.info()``  \n",
        "``df.describe()``   \n",
        "``df.columns``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVPhPTQxDOSk"
      },
      "outputs": [],
      "source": [
        "iris.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BguaQZPKpyp"
      },
      "outputs": [],
      "source": [
        "iris.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6_SObuVKpyq"
      },
      "outputs": [],
      "source": [
        "iris.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6aRueORDOSl"
      },
      "outputs": [],
      "source": [
        "iris.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKRHqQ61DOSl"
      },
      "outputs": [],
      "source": [
        "iris.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ5TfVZxDOSl"
      },
      "outputs": [],
      "source": [
        "# Rename columns\n",
        "iris.rename(columns={'class': 'species'}, inplace=True)\n",
        "\n",
        "# Display the DataFrame with renamed columns\n",
        "iris.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQy9e7zCDOSl"
      },
      "source": [
        "## Basic Plotting\n",
        "Pandas can be used for basic plotting, but we will cover more later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRwEBbLbDOSl"
      },
      "outputs": [],
      "source": [
        "iris['sepal_length'].plot.hist(bins=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE7FvE9HDOSl"
      },
      "outputs": [],
      "source": [
        "iris.plot.scatter('sepal_length','sepal_width', c='petal_width')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSs6XgaoDOSl"
      },
      "outputs": [],
      "source": [
        "iris.plot.box()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bxf2Bh-UDOSl"
      },
      "outputs": [],
      "source": [
        "iris.plot.kde()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4yFi_7DDOSl"
      },
      "source": [
        "---\n",
        "\n",
        "## Selection and Indexing\n",
        "\n",
        "There are various ways to get subsets of the data.  In the following ``df`` refers to a DataFrame.\n",
        "\n",
        "#### Selecting columns\n",
        "One column (producing a Series)\n",
        "~~~\n",
        "df['column_name']\n",
        "df.column_name\n",
        "~~~\n",
        "---\n",
        "\n",
        "Multiple columns (producing a DataFrame)\n",
        "~~~\n",
        "df[['column_name']] # this will produce a DataFrame\n",
        "df[['col1', 'col2', 'col3']]\n",
        "~~~\n",
        "---\n",
        "\n",
        "#### Selecting row and columns with ``loc`` and ``iloc``\n",
        "~~~\n",
        "df.loc['row_name', 'col_name']\n",
        "df.iloc['row index', 'col index']\n",
        "~~~\n",
        "\n",
        "``loc`` and ``iloc`` also support slicing.  Note: when slicing with ``loc``, the end point IS including (but not when slicing with ``iloc``.\n",
        "\n",
        "---\n",
        "~~~\n",
        "df.loc['row_name1':'row_name2', 'col_name1':'col_name2']\n",
        "df.loc[:, 'col_name1':'col_name2']\n",
        "df.loc['r1':'r2', :]\n",
        "df.loc[['r1','r2','r3'],['c1','c2]]\n",
        "~~~\n",
        "*When using `.loc()`, `row_name2` and `col_name2` WILL be included*\n",
        "\n",
        "---\n",
        "~~~\n",
        "df.iloc[index1:index2, col1:col2]\n",
        "~~~\n",
        "*When using `.iloc()`, `index2` and `col2` will NOT be included*\n",
        "\n",
        "---\n",
        "#### Selecting rows based on column condition\n",
        "~~~\n",
        "df[df[boolean condition]]\n",
        "\n",
        "df[mask]\n",
        "~~~\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: I usually think of iloc as integers and loc as labels, so if you are taking rows 3-5 maybe use iloc, if you are taking rows where petal_width is larger than some threshold, use loc. Also, if you want to use column names or row names, use loc."
      ],
      "metadata": {
        "id": "uduFUBqOSbz-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt64D6uJDOSl"
      },
      "outputs": [],
      "source": [
        "iris.loc[0:5, ['petal_width', 'petal_length']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLXIabquDOSl"
      },
      "outputs": [],
      "source": [
        "iris.iloc[0:2, 0:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZJ65SgDDOSl"
      },
      "outputs": [],
      "source": [
        "iris['sepal_length'] > 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po_be23cDOSl"
      },
      "outputs": [],
      "source": [
        "## Slicing with a boolean series\n",
        "iris[iris['sepal_length'] > 6]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#slicing with a boolean series with loc\n",
        "iris.loc[iris['sepal_length'] > 6]"
      ],
      "metadata": {
        "id": "fTNUGjoDS5hO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#which is faster?\n",
        "%timeit iris[iris['sepal_length'] > 6]"
      ],
      "metadata": {
        "id": "78zZ8YHLS262"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit iris.loc[iris['sepal_length'] > 6]"
      ],
      "metadata": {
        "id": "vgmiEYe9TBBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBwr3oqdDOSl"
      },
      "outputs": [],
      "source": [
        "# Filter data using multiple conditions (Note the parentheses!)\n",
        "filtered_iris = iris[(iris['sepal_length'] > 6) & (iris['petal_length'] > 5)]\n",
        "\n",
        "# Display the filtered data\n",
        "filtered_iris.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyaP7UG_DOSl"
      },
      "outputs": [],
      "source": [
        "# Reset to default 0,1...n index\n",
        "filtered_iris.reset_index(drop = True).head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#'&' obviously means and, but you can also filter using or, which is denoted with the vertical bar '|'\n",
        "\n",
        "filtered_iris2 = iris.loc[(iris.sepal_length > 6) | (iris.petal_length) < 5]\n",
        "filtered_iris2.head()"
      ],
      "metadata": {
        "id": "kVkFOXwDTKrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Fj2UddtHN3X"
      },
      "source": [
        "## Multi-Index and Index Hierarchy\n",
        "\n",
        "Let us go over how to work with Multi-Index, first we'll create a quick example of what a Multi-Indexed DataFrame would look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrNrvRq3HcTw"
      },
      "outputs": [],
      "source": [
        "# Index Levels\n",
        "outside = ['G1','G1','G1','G2','G2','G2']\n",
        "inside = [1,2,3,1,2,3]\n",
        "hier_index = list(zip(outside,inside))\n",
        "hier_index = pd.MultiIndex.from_tuples(hier_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqmaCVtMHeSj"
      },
      "outputs": [],
      "source": [
        "hier_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mh7fEZyHe8D"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(np.random.randn(6,2),index=hier_index,columns=['A','B'])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaV88xiaHfhS"
      },
      "source": [
        "Now let's show how to index this! For index hierarchy we use df.loc[], if this was on the columns axis, you would just use normal bracket notation df[]. Calling one level of the index returns the sub-dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_7vEfmlHpdI"
      },
      "outputs": [],
      "source": [
        "df.loc['G1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0NfjoBWHpVs"
      },
      "outputs": [],
      "source": [
        "df.loc['G1'].loc[1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this does the same thing as the previous cell\n",
        "df.loc[('G1', 1)]"
      ],
      "metadata": {
        "id": "r9NW9fRET2-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x72aaouHpLk"
      },
      "outputs": [],
      "source": [
        "df.index.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjAgJuq4HpAR"
      },
      "outputs": [],
      "source": [
        "df.index.names = ['Group','Num']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juONH7waHozu"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UllmZaVrHf2a"
      },
      "outputs": [],
      "source": [
        "# The xs() method in pandas is used to extract a cross-section from a DataFrame or Series, so this is the same as df.loc['G1']\n",
        "df.xs('G1')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc['G1']"
      ],
      "metadata": {
        "id": "2evIS748UQew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note from Will Melville:** I almost never encounter multi-indexing in my job. I also almost never have a named row index. Anything that I want named I make into a column, and my row index I always just have numbered from 0 to n-1, which is the default option. I truly do not know if most data scientists are naming their row index and using multi-indexing or if most are like me and just use the default 0 to n-1 row index. One advantage of multiindexing/non-default indexing is it's fast to lookup a row with an index."
      ],
      "metadata": {
        "id": "A1VDEsFNUSUJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDR3GkehDOSl"
      },
      "source": [
        "## Methods for computing summary and descriptive statistics\n",
        "pandas objects have many reduction / summary statistics methods that extract a single value from the rows or columms of a DataFrame.  See Table 5-8 in *Python for Data Analysis* for a more complete list, but here are a few that are commonly used.\n",
        "\n",
        "`count`: number of non-NA values   \n",
        "`describe`: summary statistics for numerical columns   \n",
        "`min`, `max`: min and max values  \n",
        "`argmin`, `argmax`: index of min and max values (for Series only)   \n",
        "`idxmin`, `idxmax`: index or column name of min and max values  \n",
        "`sum`: sum of values  \n",
        "`cumsum` : cummulative sum\n",
        "`mean`: mean of values  \n",
        "`quantile`: quantile from 0 to 1 of values  \n",
        "`var`: (sample) variance of values  \n",
        "`std`: (sample) standard deviation of values  \n",
        "`df.corr()` and `df.cov()` will produce the correlation or covariance matrix.  Or two Series can be used to get the correlation (or covariance) with `Series1`.corr(`Series2`).\n",
        "\n",
        "Numpy functions can also be used: `np.corrcoef()`\n",
        "\n",
        "Most of these functions also take an `axis` argument which specifies whether to reduce over rows or columns: 0 for rows and 1 for columns.   \n",
        "There is also an argument `skipna` which specifies whether or not to skip missing values.  The default is True.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84wB2FoeDOSl"
      },
      "outputs": [],
      "source": [
        "iris.sepal_length.argmin()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y99_yvm5DOSp"
      },
      "outputs": [],
      "source": [
        "iris.cumsum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].corr()"
      ],
      "metadata": {
        "id": "ZUFBaHbWXMRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris.sepal_length.quantile(0.6)"
      ],
      "metadata": {
        "id": "zCvwPxe2XXbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUMM3yHoDOSp"
      },
      "source": [
        "## Unique values and value counts\n",
        "\n",
        "``df.nunique()`` or ``df['column'].nunique()``  \n",
        "\n",
        "``df.value_counts()`` or ``df['column'].value_counts()``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUTcSnT6DOSp"
      },
      "outputs": [],
      "source": [
        "iris.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4xbl1-ODOSp"
      },
      "outputs": [],
      "source": [
        "iris.species.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESR4rkimDOSp"
      },
      "source": [
        "`df.corr()` and `df.cov()` will produce the correlation or covariance matrix.  Or two Series can be used to get the correlation (or covariance) with `Series1`.corr(`Series2`).\n",
        "\n",
        "Numpy functions can also be used: `np.corrcoef()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roEKPoRSDOSp"
      },
      "outputs": [],
      "source": [
        "iris.corr(numeric_only = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris.petal_width.corr(iris.sepal_width)"
      ],
      "metadata": {
        "id": "wF7mcWrgYAga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsur0pX4DOSp"
      },
      "source": [
        "---\n",
        "## Dropping rows and columns\n",
        "\n",
        "Columns and rows can be dropped with the `.drop()` method (using `axis=1` for columns and `axis=0` (default) for rows).  This method creates a new object unless `.inplace = True` is specified.\n",
        "\n",
        "The `del` command can also be used to drop columns in place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAK4DgoUDOSp"
      },
      "outputs": [],
      "source": [
        "no_species = iris.drop('species', axis = 1)\n",
        "no_species.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHMeU8FJDOSp"
      },
      "outputs": [],
      "source": [
        "# The original is unchanged if inplace = False\n",
        "iris.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qsRj2gRDOSp"
      },
      "outputs": [],
      "source": [
        "#more intuitive than specifying the axis, you can specify the columns or the index\n",
        "iris.drop(columns = ['sepal_length', 'sepal_width'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris.drop(index = [4, 5, 6, 7])"
      ],
      "metadata": {
        "id": "9E52AxONYchn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#by default in place is False, so the previous two cells returned new objects without editing the original object\n",
        "iris"
      ],
      "metadata": {
        "id": "-dyoz87rYh8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0pjnWMmDOSp"
      },
      "source": [
        "## Adding columns\n",
        "\n",
        "Add a new column to the end of a data frame\n",
        "~~~\n",
        "df['new_col'] = value\n",
        "~~~\n",
        "\n",
        "Add a new column at a specific index\n",
        "\n",
        "`.insert(col_index, 'new_col_name', value(s))`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUvFmVgaDOSp"
      },
      "outputs": [],
      "source": [
        "iris['sum_petal_dims'] = iris['petal_length'] + iris['petal_width']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL1lm4BoDOSp"
      },
      "outputs": [],
      "source": [
        "iris.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note from Will Melville**: I forgot/didn't realize that you could insert a column at a specific index. That feels unnecessarily tricky to me because you can always just add a column at the end then reorder the columns if you really care about the order."
      ],
      "metadata": {
        "id": "e22uxh8uYtLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_order = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'sum_petal_dims', 'species']\n",
        "new_order_iris = iris[new_order]\n",
        "new_order_iris"
      ],
      "metadata": {
        "id": "20UxMkOcZBa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJHMQ9bqDOSq"
      },
      "source": [
        "## Using Apply"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note from Will Melville**: a lot of this section feels unnecessary to me. Remember in numpy we can do vectorized operations meaning we apply a function to every item in a vector at the same time. Pandas columns can be converted to numpy using .values. So you don't need to do apply you can just use numpy. It isn't better or worse to use apply (as far as I know), but I personally have never thought to do it using apply."
      ],
      "metadata": {
        "id": "V1BmVJlIZXqt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9R4Z4-TVDOSq"
      },
      "outputs": [],
      "source": [
        "iris['sepal_length'].apply(np.log)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#same thing\n",
        "np.log(iris.sepal_length)"
      ],
      "metadata": {
        "id": "8U0JnlBpZu_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bNrDTAXDOSq"
      },
      "outputs": [],
      "source": [
        "# What happened?\n",
        "iris['sepal_length'].apply(np.mean)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#not the same thing because numpy takes mean over the series\n",
        "np.mean(iris.sepal_length)"
      ],
      "metadata": {
        "id": "hfl0NpptZ1i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TGrpObFDOSq"
      },
      "outputs": [],
      "source": [
        "iris.iloc[:, 0:4].apply(np.mean)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#same thing\n",
        "iris.iloc[:, 0:4].mean(axis = 0)"
      ],
      "metadata": {
        "id": "Y8q726W3aH3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCSdNVLHDOSq"
      },
      "outputs": [],
      "source": [
        "#the .title function capitalizes the first letter of each word.\n",
        "iris['species'].apply(lambda x: x.title())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#same thing\n",
        "iris.species.str.title()"
      ],
      "metadata": {
        "id": "7aLlSJjLasTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4-TGxHFDOSq"
      },
      "outputs": [],
      "source": [
        "iris['species'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris.species.apply(lambda x: x.lower())"
      ],
      "metadata": {
        "id": "s9VX6oQGa0Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVGP50VIDOSq"
      },
      "outputs": [],
      "source": [
        "def zero_one_scale(x):\n",
        "    return (x - np.min(x)) / (np.max(x)- np.min(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpPR6uKqKpy6"
      },
      "outputs": [],
      "source": [
        "## Why does this not work?\n",
        "iris['sepal_length'].apply(zero_one_scale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyG7liKsKpy6"
      },
      "outputs": [],
      "source": [
        "## Why does this not work?\n",
        "iris['petal_length'].apply(lambda x: zero_one_scale(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITZN99CpDOSq"
      },
      "outputs": [],
      "source": [
        "#better off not using apply at all\n",
        "zero_one_scale(iris.petal_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using apply or doing the vectorized numpy version of apply should run in a similar amount of time. Apply also takes advantage of vectorization when possible. I personally prefer not to use apply and prefer instead to just use the numpy function.\n",
        "\n",
        "When you might use .apply:\n",
        "in pyspark pandas, which is a high performance parallel computing version of pandas, when you run .apply, it applies the function over the rows in parallel over all the workers available on the computer, and that is faster for large dataframes."
      ],
      "metadata": {
        "id": "GGh0FA5JbNtj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j70DQ1UeDOSq"
      },
      "source": [
        "## Missing Values\n",
        "\n",
        "**Ways to count missing values**\n",
        "~~~\n",
        "df.info()\n",
        "df.isna().sum()\n",
        "df.isna().sum(axis=0)\n",
        "~~~\n",
        "\n",
        "**Drop missing values with `.dropna()`**\n",
        "\n",
        "Calling `.dropna()` without any arguments will drop all rows with missing values\n",
        "\n",
        "Arguments:\n",
        "* `axis=1` will drop columns with missing values (default is `axis=0`)\n",
        "* `how='all'` will drop rows (or columns) if all the values are NA (default is `how='any'`)\n",
        "* `subset=` will limit na search to these specic columns (or indexes)\n",
        "    \n",
        "\n",
        "**Fill missing values with `.fillna()`**\n",
        "Arguments:\n",
        "* `value`: value used to fill.\n",
        "* `method'`: methods used to fill (forward or backward fill). Forward fill uses the last non missing value to fill until the next nonmissing value. Backward uses the next non missing value to fill backward to the previous non missing value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITwS7RGkDOSq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtJJpcCzDOSq"
      },
      "outputs": [],
      "source": [
        "# Creating a DataFrame with missing values\n",
        "missing_data = {'A': [1, 2, np.nan],\n",
        "        'B': [np.nan, 4, 6],\n",
        "        'C': [7, 8, 9]}\n",
        "\n",
        "m_df = pd.DataFrame(missing_data)\n",
        "m_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHY4b6NHDOSq"
      },
      "outputs": [],
      "source": [
        "m_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU9Q9kgyDOSq"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "m_df.isna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCyT7G1ZDOSq"
      },
      "outputs": [],
      "source": [
        "# Check how many missing values\n",
        "m_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v27TzUpgDOSq"
      },
      "outputs": [],
      "source": [
        "# Check how many missing values\n",
        "m_df.isna().sum(axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HCStCbgDOSq"
      },
      "outputs": [],
      "source": [
        "# Fill missing values\n",
        "df_filled = m_df.fillna(-1)\n",
        "df_filled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIGcsTZsDOSr"
      },
      "outputs": [],
      "source": [
        "m_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eankcnysDOSr"
      },
      "outputs": [],
      "source": [
        "# Fill with mean column value\n",
        "m_df.fillna(m_df.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F79glDWIDOSr"
      },
      "outputs": [],
      "source": [
        "# Remove rows with missing values\n",
        "df_dropped = m_df.dropna()\n",
        "\n",
        "# Display the cleaned DataFrame\n",
        "df_dropped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk6R9FHnDOSr"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNXp4rwnDOSr"
      },
      "outputs": [],
      "source": [
        "#remove columns with missing values\n",
        "df_dropped2 = m_df.dropna(axis = 1)\n",
        "df_dropped2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1eejwalDOSr"
      },
      "source": [
        "## Groupby, Aggregation\n",
        "\n",
        "### Use Titanic data example here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv9TS_UaDOSr"
      },
      "outputs": [],
      "source": [
        "## Titanic data\n",
        "# from sklearn.datasets import fetch_openml\n",
        "# dat = fetch_openml(data_id=40945, parser = 'auto')\n",
        "# titanic = dat.frame\n",
        "\n",
        "titanic = pd.read_csv('https://raw.githubusercontent.com/rhodes-byu/cs180-winter25/refs/heads/main/data/titanic.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tl34baO9DOSr"
      },
      "outputs": [],
      "source": [
        "titanic.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SMlggWJDOSr"
      },
      "outputs": [],
      "source": [
        "titanic.drop(columns = 'name', inplace = True)\n",
        "\n",
        "titanic.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTvRRlnvDOSr"
      },
      "outputs": [],
      "source": [
        "# Average age by sex\n",
        "age_by_sex = titanic.groupby('sex')['age'].mean()\n",
        "\n",
        "# Display the aggregated data\n",
        "age_by_sex\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#if you are like me and you don't like named row indices, you can set as_index = False and it'll make the grouped value a column\n",
        "age_by_sex_2 = titanic.groupby('sex', as_index = False)['age'].mean()\n",
        "\n",
        "age_by_sex_2"
      ],
      "metadata": {
        "id": "N3sVjLjQdM22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyEi4Hw0DOSr"
      },
      "outputs": [],
      "source": [
        "# Multiple Grouping Categories\n",
        "titanic.groupby(['sex', 'pclass'])['age'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#if you don't like multi-indices:\n",
        "titanic.groupby(['sex', 'pclass'], as_index = False)['age'].mean()"
      ],
      "metadata": {
        "id": "yrb8Hjnddjgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMWIodYrDOSr"
      },
      "outputs": [],
      "source": [
        "# Multiple Target Variables\n",
        "titanic.groupby(['sex'])[['age', 'fare']].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_U7b4rMMDOSr"
      },
      "outputs": [],
      "source": [
        "# Multiple Aggregations\n",
        "titanic.groupby('sex')['age'].agg(['mean', 'max', 'min', 'sum']).round()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9Lr7yTbDOSr"
      },
      "outputs": [],
      "source": [
        "# Define a custom aggregation function\n",
        "def range(series):\n",
        "    return series.max() - series.min()\n",
        "\n",
        "titanic.groupby('sex')['fare'].agg(range)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "541_jlcGDOSr"
      },
      "outputs": [],
      "source": [
        "# Group data by 'Region' and apply named aggregations to multiple columns\n",
        "region_summary = titanic.groupby('home.dest').agg(\n",
        "    total_fare=('fare', 'sum'),\n",
        "    agerage_fare=('fare', 'mean'),\n",
        "    average_age=('age', 'mean')\n",
        ")\n",
        "\n",
        "# Display the summary for each region\n",
        "region_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the previous cell is how I most often do group by and agg, except I would've written it like this\n",
        "\n",
        "region_summary = titanic.groupby('home.dest', as_index = False).agg(total_fare = ('fare', 'sum'),\n",
        "                                                                    average_fare = ('fare', 'mean'),\n",
        "                                                                    average_age = ('age', 'mean'))\n",
        "region_summary"
      ],
      "metadata": {
        "id": "q-t2iIbSeADq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#similar, if I wanted to know the number of survivors and survivor rate of each sex/class, I would do this\n",
        "titanic.groupby(['sex', 'pclass'], as_index = False).agg(num_survivors = ('survived', 'sum'),\n",
        "                                                         survival_rate = ('survived', 'mean'))"
      ],
      "metadata": {
        "id": "k-R10Mz-eUMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL8GvVyeDOSs"
      },
      "source": [
        "### Combining DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJnhwnIPDOSs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create two DataFrames\n",
        "df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\n",
        "                    'B': ['B0', 'B1', 'B2']})\n",
        "\n",
        "df2 = pd.DataFrame({'A': ['A3', 'A4', 'A5'],\n",
        "                    'B': ['B3', 'B4', 'B5']})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QmwHAhoKpzA"
      },
      "outputs": [],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEqHOA49KpzA"
      },
      "outputs": [],
      "source": [
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2fDy0XQKpzB"
      },
      "outputs": [],
      "source": [
        "# Concatenate DataFrames vertically\n",
        "result = pd.concat([df1, df2], axis=0)\n",
        "\n",
        "# Display the concatenated DataFrame\n",
        "print(\"Concatenated DataFrame:\\n\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB4TMeu4DOSs"
      },
      "outputs": [],
      "source": [
        "# Create two DataFrames with a common column 'key'\n",
        "left = pd.DataFrame({'key': ['A', 'B', 'C'],\n",
        "                     'value_left': [1, 2, 3]})\n",
        "\n",
        "right = pd.DataFrame({'key': ['B', 'C', 'D'],\n",
        "                      'value_right': [4, 5, 6]})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJN2Ntk8KpzB"
      },
      "outputs": [],
      "source": [
        "left"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79f5kVhyKpzB"
      },
      "outputs": [],
      "source": [
        "right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wStB3UGsKpzC"
      },
      "outputs": [],
      "source": [
        "# Merge DataFrames based on the 'key' column\n",
        "merged_inner = pd.merge(left, right, on='key', how='inner')\n",
        "\n",
        "# Display the merged DataFrame\n",
        "print(\"Inner Merge:\\n\", merged_inner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oqj6QadDOSs"
      },
      "outputs": [],
      "source": [
        "merged_outer = pd.merge(left, right, on='key', how='outer')\n",
        "print(\"Outer Merge:\\n\", merged_outer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5ib4cMZDOSs"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame with wide-format data\n",
        "data = {'Name': ['Alice', 'Bob', 'Charlie'],\n",
        "        'Math_Score': [90, 85, 78],\n",
        "        'Science_Score': [88, 92, 80]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMRX07J9KpzD"
      },
      "source": [
        "### Reshaping DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIUw0Aw4DOSs"
      },
      "outputs": [],
      "source": [
        "# Melt the DataFrame to long-format\n",
        "melted_df = pd.melt(df, id_vars=['Name'], var_name='Subject', value_name='Score')\n",
        "\n",
        "# Display the melted DataFrame\n",
        "print(\"Melted DataFrame:\\n\", melted_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl60QRePDOSs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define a mapping function to assign letter grades\n",
        "def assign_grade(score):\n",
        "    if score >= 90:\n",
        "        return 'A'\n",
        "    elif score >= 80:\n",
        "        return 'B'\n",
        "    elif score >= 70:\n",
        "        return 'C'\n",
        "    else:\n",
        "        return 'F'\n",
        "\n",
        "# Apply the mapping function to create a new column 'Grade'\n",
        "melted_df['Grade'] = melted_df['Score'].map(assign_grade)\n",
        "\n",
        "# Display the DataFrame with letter grades\n",
        "print(melted_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mLInKOxDOSs"
      },
      "outputs": [],
      "source": [
        "data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Scott', 'Liz'],\n",
        "        'Age': [28, 45, 60, 34, 50, 40]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define bin edges and labels for age groups\n",
        "bin_edges = [0, 30, 40, 50, 100]\n",
        "bin_labels = ['0-30', '31-40', '41-50', '51+']\n",
        "\n",
        "# Use the `cut` function to create a new column 'AgeGroup'\n",
        "df['AgeGroup'] = pd.cut(df['Age'], bins=bin_edges, labels=bin_labels)\n",
        "\n",
        "# Display the DataFrame with age groups\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNvCasIgKpzE"
      },
      "source": [
        "### Reading in Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKPVYsqdJ9tA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CylNK4RKJ9tA"
      },
      "outputs": [],
      "source": [
        "# Make the data directory if it doesn't exist\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IpIqWX_KpzE"
      },
      "outputs": [],
      "source": [
        "# Write DataFrame to a CSV file\n",
        "df.to_csv('data/data.csv', index=False)\n",
        "\n",
        "# Read data from a CSV file\n",
        "new_df = pd.read_csv('data/data.csv')\n",
        "\n",
        "# Display the DataFrame\n",
        "print(new_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSAQVNW-J9tB"
      },
      "source": [
        "### Read in CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7krctQi6J9tB"
      },
      "outputs": [],
      "source": [
        "# Downloading data to data/ directory (May not work on Windows)\n",
        "!curl -L -o data/example_csv.csv https://raw.githubusercontent.com/rhodes-byu/stat386-datasets/refs/heads/main/reading_examples/example_csv.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdhbVSZGJ9tB"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/example_csv.csv', index_col = 0, thousands = ',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rro4u6oPJ9tB"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "None of the read sections worked for me because of the curl cells, which I don't understand. I also don't think it's all that important. The larger takeaway from the following sections is if you have a file formatted in excel, json, html, csv, txt, etc. there are ways to read it into pandas. That's all you need to know."
      ],
      "metadata": {
        "id": "RhmG8iInfJm4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtVGxKvVJ9tB"
      },
      "source": [
        "## read_excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JqXvBWCJ9tC"
      },
      "outputs": [],
      "source": [
        "!curl -L -o data/example_excel.xlsx https://raw.githubusercontent.com/rhodes-byu/stat386-datasets/refs/heads/main/reading_examples/example_excel.xlsx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZAwv_SWJ9tC"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel('data/example_excel.xlsx', sheet_name=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb9zOBXqJ9tC"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJGlus8sJ9tC"
      },
      "outputs": [],
      "source": [
        "df.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaVmkQrDJ9tC"
      },
      "outputs": [],
      "source": [
        "lines = df['lines']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-2gpvyKJ9tC"
      },
      "outputs": [],
      "source": [
        "lines.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kd8a1iqKJ9tC"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel('data/example_excel.xlsx', sheet_name = 'lines')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mto-J9-xJ9tC"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSbf52uAJ9tC"
      },
      "source": [
        "## json files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VpRqf0SJ9tC"
      },
      "outputs": [],
      "source": [
        "!curl -L -o data/example_json.json https://raw.githubusercontent.com/rhodes-byu/stat386-datasets/refs/heads/main/reading_examples/example_json.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md9d8S3JJ9tC"
      },
      "outputs": [],
      "source": [
        "pd.read_json('data/example_json.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzytMS8TJ9tC"
      },
      "outputs": [],
      "source": [
        "with open('data/example_json.json', 'r') as file:\n",
        "    json_object = json.load(open('data/example_json.json', 'r'))\n",
        "\n",
        "print(json_object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCehVkh-J9tD"
      },
      "outputs": [],
      "source": [
        "json_object['cap']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMq4T45KJ9tD"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(json_object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuJNsM8wJ9tD"
      },
      "outputs": [],
      "source": [
        "pd.json_normalize(json_object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYnnG1I3J9tD"
      },
      "source": [
        "## read_html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd25hAu0J9tD"
      },
      "outputs": [],
      "source": [
        "url = 'https://en.wikipedia.org/wiki/List_of_Super_Bowl_champions'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3cjXcOPJ9tD"
      },
      "outputs": [],
      "source": [
        "# Reads in a list of DataFrames from the URL (based on tables)\n",
        "dfs = pd.read_html(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrcgXcaFJ9tD"
      },
      "outputs": [],
      "source": [
        "len(dfs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrJPkoIwJ9tD"
      },
      "outputs": [],
      "source": [
        "dfs[9].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-ZuchSIJ9tD"
      },
      "outputs": [],
      "source": [
        "pd.read_html(url, match = 'Joe Robbie')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcrF4tJBJ9tD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read from google drive in colab:"
      ],
      "metadata": {
        "id": "F7Jfv8DWfd-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')\n",
        "\n",
        "#read in the file using the location in drive\n",
        "df = pd.read_csv('/drive/file_location_in_drive/df.csv')"
      ],
      "metadata": {
        "id": "OOX9ken2fjN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVlyaZW9J9tD"
      },
      "source": [
        "## Getting multiple pieces of information from a single column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEWLC0S0J9tD"
      },
      "source": [
        "### Unpacking\n",
        "\n",
        "Many times, a single column will contain multiple pieces of information.  Learning how to extract this information is extremely important and is a great skill to have.\n",
        "\n",
        "If it is possibe to somehow separate or split the elements in the column, this is a much easier and more effecive way of extracting information than simply extracting info based on slicing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO3CV3QrJ9tE"
      },
      "source": [
        "For example, suppose we have a list of cities with the state.  We want to separate the city and the state into individual columns.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZxuCL_9J9tE"
      },
      "outputs": [],
      "source": [
        "cities = pd.Series(['Provo, Utah', 'Omaha, Nebraska', 'Fremont, Ohio','Green River, Wyoming', 'Durham, North Carolina' ])\n",
        "cities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9GFGUXgJ9tE"
      },
      "outputs": [],
      "source": [
        "for name in cities:\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWY73wqzJ9tE"
      },
      "source": [
        "This looks like a hard problem because there are different lengths for each city and state name.  Some of the city names and state names even have spaces.  We recognize that there is a common format.  The city names are all separated from the state name by a comma.  We can use the string method ``.split(\"character\")`` to separate the words in a string based on ``\"character\"``.  \n",
        "\n",
        "By default, ``.split()`` will separate on spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74kIekyVJ9tE"
      },
      "outputs": [],
      "source": [
        "s = 'Provo, Utah'\n",
        "s.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0tQsE0GJ9tE"
      },
      "outputs": [],
      "source": [
        "s.split(\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjJYfDduJ9tE"
      },
      "outputs": [],
      "source": [
        "cities.apply(lambda x: x.split(\",\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyL-2sYNJ9tE"
      },
      "source": [
        "Or we could use  ``.str`` with ``.split``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeLRzxBEJ9tE"
      },
      "outputs": [],
      "source": [
        "cities.str.split(\",\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSVjAuHJ9tE"
      },
      "source": [
        "Now we have a list of lists.  Next we need the get the information out.  We know that our Series had only one comma and when we split on the comma (using ``.split(\",\")``) everything before the comma is the first item in the list and everything after the comma is the second item in the list.  \n",
        "In our example, the first item is the city name and the second item is the state name.\n",
        "\n",
        "Here are a couple of ways to extract the data that was split.\n",
        "\n",
        "**First using a ``for`` loop:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue3STQuEJ9tE"
      },
      "source": [
        "Notice that the state variable has white space, so we can strip that inside our for loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NaSeGqAJ9tE"
      },
      "outputs": [],
      "source": [
        "# for loop\n",
        "cities_split = cities.str.split(\",\")\n",
        "\n",
        "state = []\n",
        "city = []\n",
        "for item in cities_split:\n",
        "    city.append(item[0].strip())\n",
        "    state.append(item[1].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOK4wsLlJ9tE"
      },
      "outputs": [],
      "source": [
        "cities_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDyctHX8J9tF"
      },
      "outputs": [],
      "source": [
        "state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G44Ue1aJ9tF"
      },
      "outputs": [],
      "source": [
        "city"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cvTy1HbJ9tF"
      },
      "source": [
        "**Second using ``.apply`` and ``lambda`` functions:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idHAgVamJ9tF"
      },
      "outputs": [],
      "source": [
        "# apply with lambda function\n",
        "city = cities_split.apply(lambda x:x[0].strip())\n",
        "state = cities_split.apply(lambda x:x[1].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8zrZt1SJ9tF"
      },
      "outputs": [],
      "source": [
        "state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XDPRcDyJ9tF"
      },
      "outputs": [],
      "source": [
        "city"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAZkkA6BJ9tF"
      },
      "source": [
        "**Another Example**\n",
        "\n",
        "Here, suppose I have times in the format ``hour:minute:second``.  I want to make a variable that combines these into just one time.  Since the lowest resolution is seconds, I will make a variable for \"seconds\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIafFq4-J9tF"
      },
      "outputs": [],
      "source": [
        "times = pd.Series(['01:34:07','00:35:12','00:00:16','03:59:00'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4w1O8zFJ9tF"
      },
      "outputs": [],
      "source": [
        "time_list = times.str.split(\":\")\n",
        "time_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUaPSIixJ9tF"
      },
      "outputs": [],
      "source": [
        "seconds = []\n",
        "for time in time_list:\n",
        "    temp = int(time[0])*60*60 + int(time[1])*60 + int(time[2])\n",
        "    seconds.append(temp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4zhQUORJ9tF"
      },
      "outputs": [],
      "source": [
        "seconds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lambda function instead of for loop\n",
        "seconds = time_list.apply(lambda x: int(x[0])*60*60 + int(x[1]) * 60 + int(x[2]))\n",
        "seconds"
      ],
      "metadata": {
        "id": "n0dYH7qpgwzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7KHW4XFRg_ve"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "K4aDYI6hDOSk",
        "WQy9e7zCDOSl",
        "B4yFi_7DDOSl",
        "4Fj2UddtHN3X",
        "rDR3GkehDOSl",
        "PUMM3yHoDOSp",
        "lsur0pX4DOSp",
        "r0pjnWMmDOSp",
        "vJHMQ9bqDOSq",
        "j70DQ1UeDOSq"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}